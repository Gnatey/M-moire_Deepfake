import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import os
from datetime import datetime
from scipy.stats import chi2_contingency
import plotly.graph_objects as go
from PIL import Image
import networkx as nx
import matplotlib.pyplot as plt
from PIL import Image
from fpdf import FPDF
import streamlit as st
import io
import kaleido
import uuid
import hashlib
import json
from google.oauth2.service_account import Credentials
import gspread
import statsmodels.api as sm
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import (confusion_matrix, classification_report, roc_curve, roc_auc_score,precision_recall_curve)
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import plotly.graph_objects as go
from sklearn.utils import resample
import shap
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import MultiLabelBinarizer

# =============================================
# INITIALISATION ET CONFIGURATION DE BASE
# =============================================
st.set_page_config(
    page_title="Dashboard DeepFakes",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =============================================
# FONCTIONS UTILITAIRES
# =============================================
def local_css(file_name):
    """Charge un fichier CSS personnalis√©"""
    try:
        with open(file_name) as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
    except FileNotFoundError:
        st.markdown("""
        <style>
            .stMetric { border-radius: 10px; padding: 15px; background-color: #f8f9fa; }
            .stMetric label { font-size: 0.9rem; color: #6c757d; }
            .stMetric div { font-size: 1.4rem; font-weight: bold; color: #212529; }
            .stPlotlyChart { border: 1px solid #e1e4e8; border-radius: 8px; }
        </style>
        """, unsafe_allow_html=True)

def calculate_cramers_v(contingency_table):
    """Calcule le coefficient Cramer's V pour les tableaux de contingence"""
    chi2 = chi2_contingency(contingency_table)[0]
    n = contingency_table.sum().sum()
    phi2 = chi2/n
    r, k = contingency_table.shape
    return np.sqrt(phi2/min((k-1),(r-1)))

def truncate_label(text, max_length=25):
    """Tronque les libell√©s trop longs pour la lisibilit√©"""
    return (text[:max_length] + '...') if len(str(text)) > max_length else str(text)

# =============================================
# CHARGEMENT DES DONN√âES
# =============================================
@st.cache_data
def load_data():
    """Charge et pr√©pare les donn√©es avec gestion des erreurs"""
    url = 'https://raw.githubusercontent.com/Gnatey/M-moire_Deepfake/refs/heads/main/DeepFakes.csv'
    try:
        df = pd.read_csv(url, sep=';', encoding='utf-8')
        
        # Nettoyage des noms de colonnes
        df.columns = df.columns.str.strip()
        
        # Renommage des colonnes longues
        column_rename = {
            "Quel est votre tranche d'√¢ge ?": "Tranche d'√¢ge",
            "Vous √™tes ...?": "Genre",
            "Avez-vous d√©j√† entendu parler des Deep Fakes ?": "Connaissance DeepFakes",
            "Avez-vous d√©j√† vu un Deep Fake sur les r√©seaux sociaux ?": "Exposition DeepFakes",
            "_Sur quelles plateformes avez-vous principalement vu des Deep Fakes ? (Plusieurs choix possibles)": "Plateformes",
            "Comment √©valueriez vous votre niveau de connaissance des Deep Fakes ?": "Niveau connaissance",
            "Faites-vous confiance aux informations que vous trouvez sur les r√©seaux sociaux ?": "Confiance r√©seaux sociaux",
            "Selon vous, quel est l‚Äôimpact global des Deep Fakes sur la soci√©t√© ?": "Impact soci√©t√©"
        }
        
        return df.rename(columns=column_rename)
    
    except Exception as e:
        st.error(f"Erreur lors du chargement des donn√©es: {str(e)}")
        return pd.DataFrame()

# Chargement initial
df = load_data()
local_css("style.css")

# =============================================
# SIDEBAR FILTRES
# =============================================
with st.sidebar:
    st.header("üéõÔ∏è Filtres Principaux")
    
    if not df.empty:
        # Filtres de base
        ages = df["Tranche d'√¢ge"].dropna().unique()
        genres = df["Genre"].dropna().unique()

        # Extraction et nettoyage des plateformes individuelles
        plateforme_series = df["Plateformes"].dropna().str.split(";")
        all_plateformes = sorted(set(p.strip() for sublist in plateforme_series for p in sublist if p.strip()))

        selected_ages = st.multiselect(
            "Tranches d'√¢ge :", 
            options=ages, 
            default=ages,
            help="S√©lectionnez les tranches d'√¢ge √† inclure"
        )
        
        selected_genres = st.multiselect(
            "Genres :", 
            options=genres, 
            default=genres,
            help="Filtrez les r√©sultats par genre"
        )

        selected_plateforme = st.multiselect(
            "Plateformes :", 
            options=all_plateformes,
            default=all_plateformes,
            help="Filtrez les r√©sultats par plateformes"
        )
        
    else:
        selected_ages = []
        selected_genres = []
        selected_plateforme = []

# Application des filtres
if not df.empty:
    # Filtrage √¢ge + genre
    filtered_df = df[
        (df["Tranche d'√¢ge"].isin(selected_ages)) &
        (df["Genre"].isin(selected_genres))
    ]

    # Filtrage plateformes (ligne contenant au moins une des plateformes s√©lectionn√©es)
    if selected_plateforme:
        mask_plateformes = df["Plateformes"].dropna().apply(
            lambda x: any(p.strip() in selected_plateforme for p in x.split(";"))
        )
        filtered_df = filtered_df[mask_plateformes]
else:
    filtered_df = pd.DataFrame()


# =============================================
# ONGLETS PRINCIPAUX
# =============================================
st.title("üìä Dashboard Analyse des DeepFakes")
tab1, tab2,tab3,tab4 = st.tabs(["üè† Analyse exploratoire (EDA)", "üî¨ Exploration Avanc√©e", "üìà Analyse Statistique & Machine Learning", "Personae"])

# =============================================
# ONGLET 1 - TABLEAU DE BORD PRINCIPAL
# =============================================
with tab1:
    if filtered_df.empty:
        st.warning("Aucune donn√©e disponible avec les filtres s√©lectionn√©s.")
    else:
        st.header("üîç Indicateurs Cl√©s")
        
        # M√©triques en colonnes
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_respondents = len(filtered_df)
            st.metric("Nombre de R√©pondants", total_respondents)
        
        with col2:
            aware_yes = filtered_df["Connaissance DeepFakes"].value_counts(normalize=True).get('Oui', 0) * 100
            st.metric("% Connaissance DeepFakes", f"{aware_yes:.1f}%")
        
        with col3:
            seen_yes = filtered_df["Exposition DeepFakes"].value_counts(normalize=True).get('Oui', 0) * 100
            st.metric("% Ayant vu un DeepFake", f"{seen_yes:.1f}%")
        
        with col4:
            trust_mean = filtered_df["Confiance r√©seaux sociaux"].apply(lambda x: 1 if x == 'Oui' else 0).mean() * 100
            st.metric("Confiance moyenne (r√©seaux)", f"{trust_mean:.1f}%")
        
        # Visualisations principales
        st.header("üìà Visualisations Cl√©s")
        
        # 1. Niveau de connaissance
        st.subheader("Niveau de Connaissance des DeepFakes")
        knowledge_counts = filtered_df["Niveau connaissance"].value_counts().reset_index()
        fig_knowledge = px.bar(
            knowledge_counts, 
            x="Niveau connaissance", 
            y="count", 
            text="count",
            color="Niveau connaissance",
            template="plotly_white"
        )
        st.plotly_chart(fig_knowledge, use_container_width=True)
        
        # 2. Plateformes de DeepFakes
        st.subheader("Plateformes o√π les DeepFakes sont vus")
        if "Plateformes" in filtered_df.columns:
            platform_series = filtered_df["Plateformes"].dropna().str.split(';')
            platform_flat = [item.strip() for sublist in platform_series for item in sublist]
            platform_counts = pd.Series(platform_flat).value_counts().reset_index()
            fig_platforms = px.pie(
            platform_counts, 
                names='index', 
                values='count',
                hole=0.3,
                labels={'index': 'Plateforme', 'count': 'Occurrences'},
                color_discrete_sequence=px.colors.qualitative.Alphabet
        )

            st.plotly_chart(fig_platforms, use_container_width=True)
        
        # 3. Impact per√ßu
        st.subheader("Impact per√ßu des DeepFakes")
        impact_counts = filtered_df["Impact soci√©t√©"].value_counts().reset_index()
        fig_impact = px.bar(
            impact_counts,
            x="Impact soci√©t√©",
            y="count",
            text="count",
            color="Impact soci√©t√©",
            color_discrete_sequence=px.colors.qualitative.D3
        )
        st.plotly_chart(fig_impact, use_container_width=True)
        
        # 4. Analyse crois√©e
        st.subheader("Analyse Crois√©e")
        
        # Confiance par tranche d'√¢ge
        st.markdown("**Confiance par Tranche d'√¢ge**")
        trust_age = filtered_df.groupby("Tranche d'√¢ge")["Confiance r√©seaux sociaux"].value_counts(normalize=True).unstack() * 100
        fig_trust_age = px.bar(
            trust_age,
            barmode="group",
            labels={'value': 'Pourcentage', 'variable': 'Confiance'},
            height=500,
            color_discrete_sequence=px.colors.qualitative.Plotly
        )
        st.plotly_chart(fig_trust_age, use_container_width=True)

                # 2. Distribution de l'impact per√ßu
        st.subheader("Distribution de l‚Äôimpact per√ßu")
        impact_order = ["Tr√®s n√©gatif", "N√©gatif", "Neutre", "Positif", "Tr√®s positif"]
        fig_impact_dist = px.histogram(
            filtered_df,
            x="Impact soci√©t√©",
            category_orders={"Impact soci√©t√©": impact_order},
            color="Impact soci√©t√©",
            labels={"Impact soci√©t√©": "Impact per√ßu"},
            title="Histogramme de l'impact per√ßu"
        )
        st.plotly_chart(fig_impact_dist, use_container_width=True)

        # 3. R√©partition par genre
        st.subheader("R√©partition par genre")
        genre_counts = filtered_df["Genre"].value_counts().reset_index()
        genre_counts.columns = ["Genre", "Count"]
        fig_genre = px.bar(
            genre_counts,
            x="Genre",
            y="Count",
            text="Count",
            title="Nombre de r√©pondants par genre"
        )
        st.plotly_chart(fig_genre, use_container_width=True)

        # 4. Boxplot : Impact vs Tranche d'√¢ge
        st.subheader("Impact per√ßu selon la tranche d‚Äô√¢ge")
        # encoder l‚Äôimpact pour le boxplot
        impact_map = {k: i for i, k in enumerate(impact_order)}
        df_box = filtered_df.copy()
        df_box["Impact_code"] = df_box["Impact soci√©t√©"].map(impact_map)
        fig_box = px.box(
            df_box,
            x="Tranche d'√¢ge",
            y="Impact_code",
            labels={"Impact_code": "Impact (cod√©)", "Tranche d'√¢ge": "√Çge"},
            title="Boxplot : Impact per√ßu par tranche d‚Äô√¢ge"
        )
        st.plotly_chart(fig_box, use_container_width=True)
        
        # =============================================
        # VISUALISATION GENRE VS PLATEFORMES (ONGLET 1)
        # =============================================
        st.header("üë• Genre vs Plateformes")
        
        if "Plateformes" in filtered_df.columns:
            # Expansion des plateformes
            platform_series = filtered_df[["Plateformes", "Genre"]].dropna()
            platform_series["Plateformes"] = platform_series["Plateformes"].str.split(';')
            platform_exploded = platform_series.explode("Plateformes").dropna()
            platform_exploded["Plateformes"] = platform_exploded["Plateformes"].str.strip()
            
            # Table de contingence
            cross_tab = pd.crosstab(
                platform_exploded["Genre"],
                platform_exploded["Plateformes"]
            )
            
            # Heatmap am√©lior√©e
            fig = px.imshow(
                cross_tab,
                text_auto=True,
                aspect="auto",
                color_continuous_scale='Blues',
                height=600
            )
            fig.update_layout(
                xaxis_title="Plateformes",
                yaxis_title="Genre",
                margin=dict(t=50, b=100)
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("La colonne 'Plateformes' n'est pas disponible")

        # =============================================
        # MATRICE DE CORRELATION (ONGLET 1 SEULEMENT)
        # =============================================
        st.header("üîó Matrice de Corr√©lation")
        
        # S√©lection des colonnes pertinentes
        selected_cols = [
            "Connaissance DeepFakes",
            "Niveau connaissance", 
            "Confiance r√©seaux sociaux",
            "Impact soci√©t√©",
            "Tranche d'√¢ge",
            "Genre"
        ]
        
        # V√©rification que les colonnes existent
        if all(col in filtered_df.columns for col in selected_cols):
            df_corr = filtered_df[selected_cols].copy()
            
            # Conversion des cat√©gories en codes num√©riques
            for col in df_corr.columns:
                df_corr[col] = df_corr[col].astype('category').cat.codes
            
            # Calcul de la matrice de corr√©lation
            corr_matrix = df_corr.corr()
            
            # Labels courts pour les axes
            short_labels = {
                "Connaissance DeepFakes": "Connaissance DF",
                "Niveau connaissance": "Niveau Connaissance",
                "Confiance r√©seaux sociaux": "Confiance RS",
                "Impact soci√©t√©": "Impact Soci√©t√©",
                "Tranche d'√¢ge": "√Çge",
                "Genre": "Genre"
            }
            
            # Visualisation avec Plotly
            fig_corr = px.imshow(
                corr_matrix,
                text_auto=True,
                color_continuous_scale='RdBu',
                zmin=-1,
                zmax=1,
                labels=dict(color="Corr√©lation"),
                x=[short_labels.get(col, col) for col in corr_matrix.columns],
                y=[short_labels.get(col, col) for col in corr_matrix.index],
                aspect="auto",
                title="Matrice de Corr√©lation (Variables Pertinentes)"
            )
            
            fig_corr.update_layout(
                width=800,
                height=600,
                xaxis_tickangle=-45,
                font=dict(size=12),
                margin=dict(t=50, b=100)
            )
            
            st.plotly_chart(fig_corr, use_container_width=True)
        else:
            st.warning("Certaines colonnes n√©cessaires pour la matrice de corr√©lation sont manquantes")

# =============================================
# FONCTIONS POUR TELECHARGER L'ONGLET 1
# =============================================

def fig_to_image(fig):
    """Convertit une figure Plotly en image PNG"""
    try:
        img_bytes = fig.to_image(format="png", width=1200, height=800, scale=2)
        return Image.open(io.BytesIO(img_bytes))
    except Exception as e:
        st.error(f"Erreur conversion figure: {str(e)}")
        return None

def generate_dashboard_pdf(figures, titles):
    """G√©n√®re un PDF avec toutes les visualisations"""
    try:
        # Cr√©ation d'un buffer en m√©moire pour le PDF
        pdf_buffer = io.BytesIO()
        
        pdf = FPDF()
        pdf.set_auto_page_break(auto=True, margin=15)
        
        # M√©tadonn√©es du PDF
        pdf.set_title("Dashboard DeepFakes")
        pdf.set_author("Streamlit App")
        
        # Titre principal
        pdf.add_page()
        pdf.set_font('Arial', 'B', 16)
        pdf.cell(0, 10, 'Tableau de Bord DeepFakes - Onglet 1', 0, 1, 'C')
        pdf.ln(10)
        
        # Ajout des visualisations avec leurs titres
        for i, (fig, title) in enumerate(zip(figures, titles)):
            if fig is not None:
                # Ajout du titre de la section
                pdf.set_font('Arial', 'B', 12)
                pdf.multi_cell(0, 10, f"{i+1}. {title}")
                pdf.ln(5)
                
                # Sauvegarde temporaire de l'image
                img_path = f"temp_{uuid.uuid4()}.png"
                fig.save(img_path)
                
                # Ajout de l'image au PDF
                pdf.image(img_path, x=10, w=190)
                pdf.ln(10)
                
                # Suppression du fichier temporaire
                try:
                    os.remove(img_path)
                except:
                    pass
        
        # Pied de page
        pdf.set_font('Arial', 'I', 8)
        pdf.cell(0, 10, f"G√©n√©r√© le {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", 0, 0, 'C')
        
        # √âcriture directe dans le buffer
        output_data = pdf.output(dest='S')
        if isinstance(output_data, (bytes, bytearray)):
            pdf_buffer.write(output_data)
        else:
            pdf_buffer.write(output_data.encode('latin1'))
            pdf_buffer.seek(0)
        
        return pdf_buffer.getvalue()
    
    except Exception as e:
        st.error(f"Erreur g√©n√©ration PDF: {str(e)}")
        return None

# =============================================
# TELECHARGER TOUT L'ONGLET 1 (IMPLEMENTATION FINALE)
# =============================================
with tab1:
    if not filtered_df.empty:
        # Titres des sections
        section_titles = [
            "Niveau de Connaissance des DeepFakes",
            "Plateformes o√π les DeepFakes sont vus",
            "Impact per√ßu des DeepFakes",
            "Confiance par Tranche d'√¢ge",
            "Genre vs Plateformes",
            "Matrice de Corr√©lation"
        ]
        
        # Figures correspondantes
        figures_to_export = [
            fig_knowledge,
            fig_platforms,
            fig_impact,
            fig_trust_age,
            fig,        # heatmap Genre x Plateformes
            fig_corr    # matrice de corr√©lation
        ]
        
        # V√©rification des figures disponibles
        available_figures = [f for f in figures_to_export if f is not None]
        
        if st.button("üì• T√©l√©charger le Tableau de Bord en PDF"):
            if len(available_figures) == 0:
                st.warning("Aucune visualisation disponible √† exporter")
            else:
                with st.spinner("G√©n√©ration du PDF en cours..."):
                    # Convertir les figures en images
                    images = []
                    for fig in available_figures:
                        img = fig_to_image(fig)
                        if img is not None:
                            images.append(img)
                    
                    if images:
                        # G√©n√©rer le PDF
                        pdf_bytes = generate_dashboard_pdf(images, section_titles[:len(images)])
                        
                        if pdf_bytes:
                            # Proposer le t√©l√©chargement
                            st.download_button(
                                label="‚¨áÔ∏è T√©l√©charger le PDF",
                                data=pdf_bytes,
                                file_name="dashboard_deepfakes.pdf",
                                mime="application/pdf",
                                key="download_pdf"
                            )

# =============================================
# ONGLET 2 - EXPLORATION AVANC√âE
# =============================================
with tab2:
    st.header("‚öñÔ∏è M√©thodologie & Validit√© Scientifique")
    
    # Section de configuration avanc√©e
    with st.expander("‚öôÔ∏è Param√®tres Avanc√©s", expanded=True):
        col_config1, col_config2, col_config3 = st.columns(3)
        
        # Colonnes cat√©gorielles disponibles
        categorical_columns = [col for col in df.select_dtypes(include='object').columns 
                             if df[col].nunique() <= 15 and col in df.columns]
        
        with col_config1:
            x_axis = st.selectbox(
                "Axe X (Cat√©gorie principale)", 
                options=categorical_columns, 
                index=categorical_columns.index("Connaissance DeepFakes") if "Connaissance DeepFakes" in categorical_columns else 0,
                help="Variable pour l'axe horizontal"
            )
        
        with col_config2:
            y_axis = st.selectbox(
                "Axe Y (Sous-cat√©gorie)", 
                options=categorical_columns, 
                index=categorical_columns.index("Exposition DeepFakes") if "Exposition DeepFakes" in categorical_columns else 1,
                help="Variable pour segmenter les donn√©es"
            )
        
        with col_config3:
            color_by = st.selectbox(
                "Couleur (D√©tail)", 
                options=categorical_columns, 
                index=categorical_columns.index("Genre") if "Genre" in categorical_columns else 2,
                help="Variable pour le codage couleur"
            )
        
        # Options suppl√©mentaires
        st.markdown("---")
        col_opt1, col_opt2, col_opt3 = st.columns(3)
        
        with col_opt1:
            chart_type = st.selectbox(
                "Type de visualisation :",
                options=["Barres", "Sunburst", "Treemap", "Heatmap", "R√©seau"],
                index=0,
                help="Choisissez le type de graphique"
            )
            
        with col_opt2:
            show_percentage = st.checkbox(
                "Afficher les pourcentages", 
                True,
                help="Convertir les counts en pourcentages"
            )
            
        with col_opt3:
            min_count = st.slider(
                "Filtre count minimum", 
                min_value=1, 
                max_value=50, 
                value=5,
                help="Exclure les cat√©gories trop petites"
            )
    
    # Pr√©paration des donn√©es
    if not filtered_df.empty:
        filtered_data = filtered_df[[x_axis, y_axis, color_by]].dropna()
        cross_data = filtered_data.groupby([x_axis, y_axis, color_by]).size().reset_index(name='Count')
        
        # Application du filtre minimum
        cross_data = cross_data[cross_data['Count'] >= min_count]
        
        # Conversion en pourcentages si demand√©
        if show_percentage:
            total = cross_data['Count'].sum()
            cross_data['Count'] = (cross_data['Count'] / total * 100).round(1)
    
    # Section d'analyse statistique
    with st.expander("üìä Analyse Statistique", expanded=False):
        if not filtered_df.empty:
            contingency_table = pd.crosstab(filtered_df[x_axis], filtered_df[y_axis])
            
            if contingency_table.size > 0:
                chi2, p, dof, expected = chi2_contingency(contingency_table)
                cramers_v = calculate_cramers_v(contingency_table)
                
                st.markdown(f"""
                **Test du Chi2 d'ind√©pendance**
                - p-value = `{p:.4f}`  
                - Degr√©s de libert√© = `{dof}`  
                - Significatif √† 5%? `{"‚úÖ Oui" if p < 0.05 else "‚ùå Non"}`  
                - Coefficient Cramer's V = `{cramers_v:.3f}`
                """)
            else:
                st.warning("Table de contingence trop petite pour l'analyse")
        else:
            st.warning("Aucune donn√©e disponible pour l'analyse")
    
    # Visualisation dynamique
    if not filtered_df.empty:
        with st.spinner("G√©n√©ration de la visualisation..."):
            try:
                if chart_type == "Barres":
                    # Pr√©paration des libell√©s
                    cross_data[x_axis] = cross_data[x_axis].apply(truncate_label)
                    cross_data[y_axis] = cross_data[y_axis].apply(truncate_label)
                    cross_data[color_by] = cross_data[color_by].apply(truncate_label)
                    
                    fig = px.bar(
                        cross_data,
                        x=x_axis,
                        y='Count',
                        color=color_by,
                        barmode='group',
                        text='Count',
                        facet_col=y_axis,
                        title=f"<b>{x_axis} vs {y_axis} par {color_by}</b>",
                        labels={'Count': "Nombre" if not show_percentage else "Pourcentage"},
                        color_discrete_sequence=px.colors.qualitative.Pastel
                    )
                    
                    fig.update_layout(
                        height=600,
                        width=max(800, len(cross_data)*20),
                        xaxis_tickangle=-45,
                        yaxis_title="Nombre" if not show_percentage else "Pourcentage (%)",
                        legend_title=color_by,
                        margin=dict(t=100)
                    )
                    
                    fig.update_traces(
                        textposition='outside',
                        texttemplate='%{text}' + ('%' if show_percentage else '')
                    )
                
                elif chart_type == "Sunburst":
                    fig = px.sunburst(
                        cross_data,
                        path=[x_axis, y_axis, color_by],
                        values='Count',
                        title=f"<b>Hi√©rarchie: {x_axis} > {y_axis} > {color_by}</b>",
                        color_discrete_sequence=px.colors.qualitative.Pastel
                    )
                    
                    fig.update_traces(
                        textinfo="label+percent parent",
                        hovertemplate="<b>%{label}</b><br>" + 
                                     ("Count: %{value}<br>" if not show_percentage else "Pourcentage: %{value}%<br>") + 
                                     "%{percentParent:.1%} of parent"
                    )
                
                elif chart_type == "Treemap":
                    fig = px.treemap(
                        cross_data,
                        path=[x_axis, y_axis, color_by],
                        values='Count',
                        title=f"<b>R√©partition: {x_axis} > {y_axis} > {color_by}</b>",
                        color_discrete_sequence=px.colors.qualitative.Pastel
                    )
                    
                    fig.update_traces(
                        textinfo="label+value+percent parent",
                        texttemplate="<b>%{label}</b><br>" + 
                                    ("%{value}" if not show_percentage else "%{value}%") + 
                                    "<br>%{percentParent:.1%}"
                    )
                
                elif chart_type == "Heatmap":
                    pivot_data = cross_data.pivot_table(
                        index=x_axis,
                        columns=y_axis,
                        values='Count',
                        aggfunc='sum',
                        fill_value=0
                    )
                    
                    fig = px.imshow(
                        pivot_data,
                        labels=dict(x=y_axis, y=x_axis, color="Count"),
                        title=f"<b>Heatmap: {x_axis} vs {y_axis}</b>",
                        aspect="auto",
                        color_continuous_scale='Blues',
                        text_auto=True
                    )
                
                elif chart_type == "R√©seau":
                    # Cr√©ation du graphique r√©seau
                    G = nx.from_pandas_edgelist(
                        cross_data, 
                        source=x_axis, 
                        target=y_axis, 
                        edge_attr='Count'
                    )
                    
                    pos = nx.spring_layout(G)
                    edge_x = []
                    edge_y = []
                    for edge in G.edges():
                        x0, y0 = pos[edge[0]]
                        x1, y1 = pos[edge[1]]
                        edge_x.extend([x0, x1, None])
                        edge_y.extend([y0, y1, None])
                    
                    edge_trace = go.Scatter(
                        x=edge_x, y=edge_y,
                        line=dict(width=0.5, color='#888'),
                        hoverinfo='none',
                        mode='lines'
                    )
                    
                    node_x = []
                    node_y = []
                    for node in G.nodes():
                        x, y = pos[node]
                        node_x.append(x)
                        node_y.append(y)
                    
                    node_trace = go.Scatter(
                        x=node_x, y=node_y,
                        mode='markers+text',
                        hoverinfo='text',
                        marker=dict(
                            showscale=True,
                            colorscale='YlGnBu',
                            size=10,
                            colorbar=dict(
                                thickness=15,
                                title='Connections',
                                xanchor='left',
                                titleside='right'
                            )
                        )
                    )
                    
                    fig = go.Figure(
                        data=[edge_trace, node_trace],
                        layout=go.Layout(
                            title=f'<br>R√©seau: {x_axis} ‚Üî {y_axis}',
                            showlegend=False,
                            hovermode='closest',
                            margin=dict(b=20,l=5,r=5,t=40),
                            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
                        )
                    )
                
                # Affichage du graphique
                st.plotly_chart(fig, use_container_width=True)
                pdf_buffer = io.BytesIO()
                fig.write_image(pdf_buffer, format="pdf")
                pdf_buffer.seek(0)
                
                # Options d'export
                st.markdown("---")
                col_export1, col_export2 = st.columns(2)
                
                with col_export1:
                    st.download_button(
                        label="üíæ T√©l√©charger le graphique en PDF",
                        data=pdf_buffer,
                        file_name="graphique_plotly.pdf",
                        mime="application/pdf"
                    )

                
                with col_export2:
                    st.download_button(
                        label="üìÑ T√©l√©charger les donn√©es",
                        data=cross_data.to_csv(index=False),
                        file_name="donnees_croisees.csv",
                        mime="text/csv"
                    )
                
            except Exception as e:
                st.error(f"Erreur lors de la g√©n√©ration du graphique : {str(e)}")
                st.warning("Veuillez s√©lectionner des combinaisons de variables compatibles")



    # =============================================
    # SECTION 1 : DESCRIPTION DE L'√âCHANTILLON
    # =============================================
    with st.expander("üìä Caract√©ristiques de l'√©chantillon", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("<div class='metric-card'>üìè <b>Taille</b><br>"
                        f"<span style='font-size:24px'>{len(df)} r√©pondants</span></div>", 
                        unsafe_allow_html=True)
            
        with col2:
            mode_recrutement = "Volontariat en ligne"
            st.markdown("<div class='metric-card'>üéØ <b>Recrutement</b><br>"
                        f"<span style='font-size:16px'>{mode_recrutement}</span></div>", 
                        unsafe_allow_html=True)
            
        with col3:
            duree_enquete = (pd.to_datetime(df['Date de saisie']).max() - 
                            pd.to_datetime(df['Date de saisie']).min()).days +1
            st.markdown("<div class='metric-card'>‚è± <b>Dur√©e</b><br>"
                        f"<span style='font-size:24px'>{duree_enquete} jours</span></div>", 
                        unsafe_allow_html=True)

        # Distribution d√©mographique
        st.subheader("R√©partition d√©mographique")
        demo_col1, demo_col2 = st.columns(2)
        
        with demo_col1:
            fig_age = px.pie(df, names="Tranche d'√¢ge", title="Distribution par √¢ge",
                            color_discrete_sequence=px.colors.sequential.RdBu)
            st.plotly_chart(fig_age, use_container_width=True)
            
        with demo_col2:
            fig_genre = px.pie(df, names="Genre", title="R√©partition par genre",
                              color_discrete_sequence=px.colors.qualitative.Pastel)
            st.plotly_chart(fig_genre, use_container_width=True)

    # =============================================
    # SECTION 2 : REPR√âSENTATIVIT√â
    # =============================================
    with st.expander("üßÆ Analyse de repr√©sentativit√©", expanded=True):
        st.subheader("Test de repr√©sentativit√©")
        
        # Charger les donn√©es INSEE (exemple simplifi√©)
        insee_data = {
            "Tranche d'√¢ge": ["18-25", "26-40", "41-60", "60+"],
            "Population (%)": [22, 35, 30, 13]
        }
        df_insee = pd.DataFrame(insee_data)
        
        # Calcul des √©carts
        df_compare = df["Tranche d'√¢ge"].value_counts(normalize=True).reset_index()
        df_compare.columns = ["Tranche d'√¢ge", "√âchantillon (%)"]
        df_compare = df_compare.merge(df_insee, on="Tranche d'√¢ge", how="left")
        df_compare["√âcart (%)"] = df_compare["√âchantillon (%)"] - df_compare["Population (%)"]
        
        # Visualisation comparative
        fig_comp = go.Figure()
        fig_comp.add_trace(go.Bar(
            x=df_compare["Tranche d'√¢ge"],
            y=df_compare["√âchantillon (%)"],
            name='Notre √©chantillon',
            marker_color='#1f77b4'
        ))
        fig_comp.add_trace(go.Bar(
            x=df_compare["Tranche d'√¢ge"],
            y=df_compare["Population (%)"],
            name='Population INSEE',
            marker_color='#ff7f0e'
        ))
        fig_comp.update_layout(barmode='group', title="Comparaison avec les donn√©es INSEE")
        st.plotly_chart(fig_comp, use_container_width=True)
        
        # Test du Chi2
        st.markdown("**Test d'ad√©quation du Chi¬≤**")
        from scipy.stats import chisquare
        observed = df_compare["√âchantillon (%)"].values * len(df) / 100
        expected = df_compare["Population (%)"].values * len(df) / 100
        chi2, p = chisquare(f_obs=observed, f_exp=expected)
        
        st.markdown(f"""
        <div class='stat-test'>
        œá¬≤ = {chi2:.3f}<br>
        p-value = {p:.4f}<br>
        <b>Conclusion</b> : {"L'√©chantillon est repr√©sentatif" if p > 0.05 else "Biais de repr√©sentativit√© d√©tect√©"}
        </div>
        """, unsafe_allow_html=True)

    # =============================================
    # SECTION 3 : INTERVALLES DE CONFIANCE
    # =============================================
    with st.expander("üì∂ Pr√©cision des estimations", expanded=True):
        st.subheader("Intervalles de confiance (bootstrap)")
        
        # Param√®tres
        col_var, col_level = st.columns(2)
        with col_var:
            target_var = st.selectbox("Variable d'int√©r√™t", 
                                    ["Connaissance DeepFakes", "Exposition DeepFakes"])
        with col_level:
            conf_level = st.slider("Niveau de confiance", 90, 99, 95)
        
        # Bootstrap
        def bootstrap_ci(data, n_bootstrap=1000):
            means = []
            for _ in range(n_bootstrap):
                sample = np.random.choice(data, size=len(data), replace=True)
                means.append(np.mean(sample == "Oui"))
            return np.percentile(means, [(100-conf_level)/2, 100-(100-conf_level)/2])
        
        ci_low, ci_high = bootstrap_ci(df[target_var])
        true_prop = (df[target_var] == "Oui").mean()
        
        # Visualisation
        fig_ci = go.Figure()
        fig_ci.add_trace(go.Indicator(
            mode="number+gauge",
            value=true_prop * 100,
            number={"suffix": "%"},
            domain={"x": [0.1, 1], "y": [0, 1]},
            gauge={
                "shape": "bullet",
                "axis": {"range": [0, 100]},
                "threshold": {
                    "line": {"color": "red", "width": 2},
                    "thickness": 0.75,
                    "value": true_prop * 100},
                "steps": [
                    {"range": [0, ci_low*100], "color": "lightgray"},
                    {"range": [ci_low*100, ci_high*100], "color": "gray"}],
                "bar": {"color": "black"}
            }))
        fig_ci.update_layout(title=f"Intervalle de confiance {conf_level}% pour {target_var}")
        st.plotly_chart(fig_ci, use_container_width=True)
        
        st.markdown(f"""
        La proportion r√©elle est de **{true_prop*100:.1f}%**  
        Intervalle de confiance : **[{ci_low*100:.1f}% - {ci_high*100:.1f}%]**
        """)

    # =============================================
    # SECTION 4 : ANALYSE DES BIAIS
    # =============================================
    with st.expander("‚ö†Ô∏è Diagnostic des biais", expanded=True):
        st.subheader("Carte des biais potentiels")

        biases = {
            "Biais de s√©lection": {
                "Description": "Sur-repr√©sentation des internautes avertis",
                "Impact": "Mod√©r√©",
                "Correctif": "Pond√©ration par calage"
            },
            "Biais de non-r√©ponse": {
                "Description": "Abandon apr√®s visualisation des questions complexes",
                "Impact": "Faible",
                "Correctif": "Analyse des r√©pondants partiels"
            },
            "Biais de d√©sirabilit√©": {
                "Description": "Sous-d√©claration des comportements risqu√©s",
                "Impact": "√âlev√©",
                "Correctif": "Donn√©es anonymis√©es"
            }
        }

        # Matrice d'√©valuation
        df_biases = pd.DataFrame(biases).T.reset_index()
        df_biases.columns = ["Type de biais", "Description", "Impact", "Correctif"]

        # Appliquer des attributs HTML personnalis√©s pour la colonne 'Impact'
        def add_data_attr(val):
            return f'data-impact="{val}"'

        styled_biases = df_biases.style.set_td_classes(
            df_biases[['Impact']].applymap(add_data_attr)
        )

        st.dataframe(
            styled_biases,
            hide_index=True,
            use_container_width=True
        )

        # Diagramme radar des risques

        fig_radar = go.Figure()
        fig_radar.add_trace(go.Scatterpolar(
            r=[3, 2, 1],  # Scores d'impact fictifs
            theta=list(biases.keys()),
            fill='toself',
            name='Impact des biais'
        ))
        fig_radar.update_layout(
            polar=dict(
                bgcolor="#ffffff",
                radialaxis=dict(
                    visible=True,
                    range=[0, 4],
                    color="#2c3e50",
                    gridcolor="#d0d0d0",
                    linecolor="#999999"
                ),
                angularaxis=dict(
                    color="white",
                    gridcolor="#d0d0d0",
                    linecolor="#999999"
                )
            ),
            font=dict(color="#2c3e50"),
            title=dict(
                text="Cartographie des biais par niveau d'impact",
                font=dict(color="white", size=20)
            )
        )

        st.plotly_chart(fig_radar, use_container_width=True)

    # =============================================
    # SECTION 5 : CONCLUSION M√âTHODOLOGIQUE
    # =============================================
    with st.container(border=True):
        st.subheader("üîé Conclusion sur la validit√©")
        
        # Score global de validit√©
        validity_score = 78  # Exemple de calcul composite
        
        st.markdown(f"""
            <div style="
            background-color: #ffffff10;
            border: 1px solid rgba(255,255,255,0.1);
            border-radius: 12px;
            padding: 1.5rem;
            box-shadow: 0 4px 10px rgba(0,0,0,0.15);
            color: #ecf0f1;
                            ">
            <h3 style="color: #ecf0f1;">Validit√© scientifique globale : {validity_score}/100</h3>
        <div style="margin-top: 10px; font-size: 0.95rem;">
            <p><b>Points forts :</b> Taille suffisante (n>={len(df)}), IC serr√©s, tests significatifs</p>
            <p><b>Limites :</b> Biais de s√©lection, couverture g√©ographique limit√©e</p>
            <p><b>G√©n√©ralisation :</b> Possible avec pond√©ration pour les √©tudes descriptives</p>
        </div>
</div>
""", unsafe_allow_html=True)

# =============================================
# ONGLET 3 - MACHINE LEARNING
# =============================================

with tab3:
    st.header("ü§ñ Machine Learning : Pr√©dire le Comportement face aux DeepFakes")
    
    # Introduction p√©dagogique
    st.markdown("""
    ### üéØ **Objectif Simple**
    Nous allons **pr√©dire** qui a d√©j√† vu un DeepFake en analysant le profil des utilisateurs.
    Avec **{} observations**, nous pouvons cr√©er un mod√®le pr√©dictif efficace !
    """.format(len(filtered_df) if not filtered_df.empty else 0))
    
    if filtered_df.empty:
        st.warning("‚ö†Ô∏è Aucune donn√©e disponible - Ajustez les filtres dans la sidebar")
        st.stop()
    
    # =============================================
    # √âTAPE 1: CHOIX DE LA CIBLE (SIMPLIFI√â)
    # =============================================
    
    st.markdown("### üìä **√âtape 1 : Que voulons-nous pr√©dire ?**")
    
    # Options de pr√©diction simplifi√©es
    prediction_options = {
        "Exposition DeepFakes": {
            "question": "Qui a d√©j√† vu un DeepFake ?",
            "why": "Identifier les profils les plus expos√©s pour cibler les campagnes de sensibilisation"
        },
        "Connaissance DeepFakes": {
            "question": "Qui conna√Æt les DeepFakes ?", 
            "why": "Comprendre qui est inform√© sur cette technologie"
        },
        "Confiance r√©seaux sociaux": {
            "question": "Qui fait confiance aux r√©seaux sociaux ?",
            "why": "Identifier les utilisateurs les plus vuln√©rables √† la d√©sinformation"
        }
    }
    
    col_choice, col_info = st.columns([1, 2])
    
    with col_choice:
        target_var = st.selectbox(
            "üéØ **Choisissez votre pr√©diction :**",
            options=list(prediction_options.keys()),
            format_func=lambda x: prediction_options[x]["question"]
        )
    
    with col_info:
        st.info(f"**Pourquoi cette pr√©diction ?**\n{prediction_options[target_var]['why']}")
    
    # V√©rification des donn√©es
    if target_var not in filtered_df.columns:
        st.error(f"‚ùå La colonne '{target_var}' n'existe pas dans vos donn√©es")
        st.stop()
    
    # =============================================
    # √âTAPE 2: ANALYSE DES DONN√âES CIBLES
    # =============================================
    
    st.markdown("### üìà **√âtape 2 : Analyse de nos donn√©es**")
    
    # Nettoyage et pr√©paration
    target_data = filtered_df[target_var].dropna()
    
    if len(target_data) < 30:
        st.error("‚ùå Pas assez de donn√©es (minimum 30 observations n√©cessaires)")
        st.stop()
    
    # Visualisation de la distribution
    col_dist, col_stats = st.columns([2, 1])
    
    with col_dist:
        target_counts = target_data.value_counts()
        fig_target = px.pie(
            values=target_counts.values,
            names=target_counts.index,
            title=f"üéØ Distribution de '{target_var}'",
            color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        )
        st.plotly_chart(fig_target, use_container_width=True)
    
    with col_stats:
        st.markdown("**üìä Statistiques :**")
        for classe, count in target_counts.items():
            pct = (count / len(target_data)) * 100
            st.metric(f"Classe '{classe}'", f"{count} ({pct:.1f}%)")
        
        # V√©rification √©quilibre
        min_class_pct = target_counts.min() / len(target_data) * 100
        if min_class_pct < 20:
            st.warning(f"‚ö†Ô∏è Classes d√©s√©quilibr√©es (min: {min_class_pct:.1f}%)")
        else:
            st.success("‚úÖ Classes √©quilibr√©es")
    
    # =============================================
    # √âTAPE 3: PR√âPARATION AUTOMATIQUE DES DONN√âES
    # =============================================
    
    def prepare_smart_dataset(df, target):
        """Pr√©paration intelligente des donn√©es pour petit dataset"""
        
        st.markdown("### üîß **√âtape 3 : Pr√©paration automatique des donn√©es**")
        
        # √âtapes de nettoyage
        with st.expander("üîç Voir les d√©tails du nettoyage", expanded=False):
            st.write("**1. S√©lection des variables pertinentes**")
            
            # Variables √† garder (les plus importantes pour DeepFakes)
            keep_cols = [
                "Tranche d'√¢ge", "Genre", "Connaissance DeepFakes", 
                "Exposition DeepFakes", "Niveau connaissance",
                "Confiance r√©seaux sociaux", "Impact soci√©t√©"
            ]
            
            # Garder seulement les colonnes qui existent
            available_cols = [col for col in keep_cols if col in df.columns and col != target]
            
            st.write(f"Variables s√©lectionn√©es : {available_cols}")
            
            # Cr√©ation du dataset
            ml_data = df[available_cols + [target]].copy()
            initial_size = len(ml_data)
            
            st.write(f"**2. Nettoyage des valeurs manquantes**")
            st.write(f"Taille initiale : {initial_size} observations")
            
            # Supprimer les lignes avec target manquant
            ml_data = ml_data.dropna(subset=[target])
            st.write(f"Apr√®s suppression target manquante : {len(ml_data)} observations")
            
            # Pour les autres colonnes, remplacer par le mode
            for col in available_cols:
                if ml_data[col].isnull().sum() > 0:
                    mode_val = ml_data[col].mode()[0] if len(ml_data[col].mode()) > 0 else "Inconnu"
                    ml_data[col] = ml_data[col].fillna(mode_val)
                    st.write(f"Colonne '{col}': {ml_data[col].isnull().sum()} valeurs manquantes remplac√©es")
            
            st.write(f"**3. R√©sultat final : {len(ml_data)} observations pr√™tes pour l'analyse**")
        
        return ml_data, available_cols
    
    # Pr√©paration des donn√©es
    ml_data, feature_cols = prepare_smart_dataset(filtered_df, target_var)
    
    if len(ml_data) < 20:
        st.error(f"‚ùå Dataset trop petit apr√®s nettoyage : {len(ml_data)} observations")
        st.stop()
    
    # =============================================
    # √âTAPE 4: MOD√àLES OPTIMIS√âS POUR PETIT DATASET
    # =============================================
    
    st.markdown("### üöÄ **√âtape 4 : Entra√Ænement des mod√®les**")
    
    # S√©lection des mod√®les adapt√©s aux petits datasets
    st.markdown("**Mod√®les s√©lectionn√©s** (optimis√©s pour {} observations) :".format(len(ml_data)))
    
    col_m1, col_m2, col_m3 = st.columns(3)
    with col_m1:
        st.info("**üéØ R√©gression Logistique**\nSimple et efficace")
    with col_m2:
        st.info("**üå≥ Random Forest**\nG√®re bien les interactions")
    with col_m3:
        st.info("**üîç SVM**\nTrouve les fronti√®res complexes")
    
    if st.button("üöÄ **LANCER L'ANALYSE COMPL√àTE**", type="primary"):
        
        # Pr√©paration train/test
        X = ml_data[feature_cols]
        y = ml_data[target_var]
        
        # Encodage des variables cat√©gorielles
        from sklearn.preprocessing import LabelEncoder
        
        # Sauvegarde des encodeurs pour l'interpr√©tation
        encoders = {}
        X_encoded = X.copy()
        
        for col in X.columns:
            if X[col].dtype == 'object':
                le = LabelEncoder()
                X_encoded[col] = le.fit_transform(X[col])
                encoders[col] = le
        
        # Encodage de la target si n√©cessaire
        y_encoded = y
        target_encoder = None
        if y.dtype == 'object':
            target_encoder = LabelEncoder()
            y_encoded = target_encoder.fit_transform(y)
        
        # Split optimis√© pour petit dataset (70/30)
        X_train, X_test, y_train, y_test = train_test_split(
            X_encoded, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
        )
        
        st.success(f"‚úÖ Donn√©es pr√©par√©es : {len(X_train)} pour entra√Ænement, {len(X_test)} pour test")
        
        # =============================================
        # MOD√àLES OPTIMIS√âS
        # =============================================
        
        from sklearn.linear_model import LogisticRegression
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.svm import SVC
        from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve
        from sklearn.preprocessing import StandardScaler
        
        # Standardisation (importante pour SVM et LogReg)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # D√©finition des mod√®les optimis√©s
        models = {
            "R√©gression Logistique": LogisticRegression(
                random_state=42, 
                max_iter=1000,
                C=0.1,  # R√©gularisation plus forte pour √©viter l'overfitting
                solver='liblinear'
            ),
            "Random Forest": RandomForestClassifier(
                n_estimators=50,  # Moins d'arbres pour √©viter l'overfitting
                max_depth=5,      # Profondeur limit√©e
                min_samples_split=10,  # Plus conservateur
                random_state=42
            ),
            "SVM": SVC(
                probability=True,
                random_state=42,
                C=0.5,  # R√©gularisation
                kernel='rbf'
            )
        }
        
        # =============================================
        # ENTRA√éNEMENT ET √âVALUATION
        # =============================================
        
        results = {}
        
        progress_bar = st.progress(0)
        for i, (name, model) in enumerate(models.items()):
            
            progress_bar.progress((i + 1) / len(models))
            
            try:
                # Choix des donn√©es (scaled pour LogReg et SVM)
                if name in ["R√©gression Logistique", "SVM"]:
                    X_train_model, X_test_model = X_train_scaled, X_test_scaled
                else:
                    X_train_model, X_test_model = X_train, X_test
                
                # Entra√Ænement
                model.fit(X_train_model, y_train)
                
                # Pr√©dictions
                y_pred = model.predict(X_test_model)
                y_pred_proba = model.predict_proba(X_test_model)
                
                # M√©triques
                accuracy = accuracy_score(y_test, y_pred)
                
                # AUC pour binaire uniquement
                auc = None
                if len(np.unique(y_encoded)) == 2:
                    auc = roc_auc_score(y_test, y_pred_proba[:, 1])
                
                results[name] = {
                    'model': model,
                    'accuracy': accuracy,
                    'auc': auc,
                    'y_pred': y_pred,
                    'y_pred_proba': y_pred_proba,
                    'X_test': X_test_model
                }
                
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erreur avec {name}: {str(e)}")
        
        # =============================================
        # VISUALISATION DES R√âSULTATS
        # =============================================
        
        st.markdown("### üèÜ **R√©sultats de Performance**")
        
        if results:
            # Tableau de r√©sultats
            perf_data = []
            for name, metrics in results.items():
                perf_data.append({
                    'Mod√®le': name,
                    'Accuracy': f"{metrics['accuracy']:.1%}",
                    'AUC': f"{metrics['auc']:.3f}" if metrics['auc'] else "N/A"
                })
            
            perf_df = pd.DataFrame(perf_data)
            
            # Affichage avec style
            col_table, col_best = st.columns([2, 1])
            
            with col_table:
                st.dataframe(
                    perf_df,
                    hide_index=True,
                    use_container_width=True
                )
            
            with col_best:
                best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
                best_accuracy = results[best_model_name]['accuracy']
                
                if best_accuracy >= 0.85:
                    st.success(f"üéâ **Objectif atteint !**\n{best_model_name}\n**{best_accuracy:.1%}**")
                elif best_accuracy >= 0.75:
                    st.info(f"üëç **Bonne performance**\n{best_model_name}\n**{best_accuracy:.1%}**")
                else:
                    st.warning(f"üìà **Performance mod√©r√©e**\n{best_model_name}\n**{best_accuracy:.1%}**")
            
            # =============================================
            # COURBE ROC (SI BINAIRE)
            # =============================================
            
            if len(np.unique(y_encoded)) == 2:
                st.markdown("### üìä **Courbe ROC : Qualit√© de Discrimination**")
                
                fig_roc = go.Figure()
                
                # Ligne de r√©f√©rence
                fig_roc.add_trace(go.Scatter(
                    x=[0, 1], y=[0, 1],
                    mode='lines',
                    line=dict(dash='dash', color='gray'),
                    name='Hasard (AUC = 0.5)',
                    hovertemplate='Ligne de hasard<extra></extra>'
                ))
                
                # Courbes pour chaque mod√®le
                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
                for i, (name, metrics) in enumerate(results.items()):
                    if metrics['auc']:
                        fpr, tpr, _ = roc_curve(y_test, metrics['y_pred_proba'][:, 1])
                        
                        fig_roc.add_trace(go.Scatter(
                            x=fpr, y=tpr,
                            mode='lines',
                            line=dict(color=colors[i], width=3),
                            name=f"{name} (AUC = {metrics['auc']:.3f})",
                            hovertemplate=f'{name}<br>FPR: %{{x:.3f}}<br>TPR: %{{y:.3f}}<extra></extra>'
                        ))
                
                fig_roc.update_layout(
                    title="Courbe ROC - Capacit√© √† distinguer les classes",
                    xaxis_title="Taux de Faux Positifs (FPR)",
                    yaxis_title="Taux de Vrais Positifs (TPR)",
                    width=700,
                    height=500,
                    hovermode='closest'
                )
                
                st.plotly_chart(fig_roc, use_container_width=True)
                
                st.info("üí° **Interpr√©tation** : Plus la courbe est proche du coin sup√©rieur gauche, meilleur est le mod√®le !")
            
            # =============================================
            # COURBES D'APPRENTISSAGE
            # =============================================
            
            st.markdown("### üìà **Courbes d'Apprentissage : √âviter le Surapprentissage**")
            
            from sklearn.model_selection import learning_curve
            
            # Prendre le meilleur mod√®le
            best_model = results[best_model_name]['model']
            
            # Donn√©es pour le meilleur mod√®le
            if best_model_name in ["R√©gression Logistique", "SVM"]:
                X_for_learning = X_encoded
                X_for_learning = scaler.fit_transform(X_for_learning)
            else:
                X_for_learning = X_encoded
            
            # Calcul des courbes d'apprentissage
            train_sizes, train_scores, val_scores = learning_curve(
                best_model, X_for_learning, y_encoded,
                train_sizes=np.linspace(0.3, 1.0, 5),
                cv=3,  # 3-fold CV pour petit dataset
                scoring='accuracy',
                random_state=42
            )
            
            # Moyennes et √©cart-types
            train_mean = train_scores.mean(axis=1)
            train_std = train_scores.std(axis=1)
            val_mean = val_scores.mean(axis=1)
            val_std = val_scores.std(axis=1)
            
            # Visualisation
            fig_learning = go.Figure()
            
            # Courbe d'entra√Ænement
            fig_learning.add_trace(go.Scatter(
                x=train_sizes,
                y=train_mean,
                mode='lines+markers',
                name='Score Entra√Ænement',
                line=dict(color='#FF6B6B', width=3),
                error_y=dict(
                    type='data',
                    array=train_std,
                    visible=True
                )
            ))
            
            # Courbe de validation
            fig_learning.add_trace(go.Scatter(
                x=train_sizes,
                y=val_mean,
                mode='lines+markers',
                name='Score Validation',
                line=dict(color='#4ECDC4', width=3),
                error_y=dict(
                    type='data',
                    array=val_std,
                    visible=True
                )
            ))
            
            fig_learning.update_layout(
                title=f"Courbes d'Apprentissage - {best_model_name}",
                xaxis_title="Nombre d'√©chantillons d'entra√Ænement",
                yaxis_title="Score d'Accuracy",
                yaxis=dict(range=[0, 1]),
                height=500
            )
            
            st.plotly_chart(fig_learning, use_container_width=True)
            
            # Diagnostic
            final_gap = train_mean[-1] - val_mean[-1]
            if final_gap < 0.05:
                st.success("‚úÖ **Bon √©quilibre** : Pas de surapprentissage d√©tect√©")
            elif final_gap < 0.15:
                st.warning("‚ö†Ô∏è **Surapprentissage mod√©r√©** : Le mod√®le pourrait √™tre simplifi√©")
            else:
                st.error("‚ùå **Surapprentissage important** : Le mod√®le m√©morise trop les donn√©es")
            
            # =============================================
            # ANALYSE D'EXPLICABILIT√â ROBUSTE
            # =============================================
            
            st.markdown("### üß† **Explicabilit√© du Mod√®le : Pourquoi ces pr√©dictions ?**")
            
            # Analyse de l'importance des variables
            if best_model_name == "Random Forest":
                rf_model = results[best_model_name]['model']
                importances = rf_model.feature_importances_
                
                # Feature importance avec statistiques
                feature_importance_df = pd.DataFrame({
                    'Variable': feature_cols,
                    'Importance': importances,
                    'Pourcentage': (importances / importances.sum()) * 100
                }).sort_values('Importance', ascending=True)
                
                # Visualisation principale
                fig_importance = px.bar(
                    feature_importance_df,
                    x='Importance',
                    y='Variable',
                    orientation='h',
                    title="üéØ Impact des Variables sur les Pr√©dictions",
                    color='Importance',
                    color_continuous_scale='Viridis',
                    text='Pourcentage'
                )
                
                fig_importance.update_traces(
                    texttemplate='%{text:.1f}%',
                    textposition='outside'
                )
                
                fig_importance.update_layout(
                    height=400,
                    xaxis_title="Importance (Contribution au Mod√®le)",
                    yaxis_title="Variables"
                )
                
                st.plotly_chart(fig_importance, use_container_width=True)
                
                # Interpr√©tation automatique
                top_variable = feature_importance_df.iloc[-1]
                second_variable = feature_importance_df.iloc[-2] if len(feature_importance_df) > 1 else None
                
                col_interp1, col_interp2 = st.columns(2)
                
                with col_interp1:
                    st.success(f"""
                    üèÜ **Variable la plus influente :**
                    **{top_variable['Variable']}** ({top_variable['Pourcentage']:.1f}% de l'impact)
                    
                    Cette variable est cruciale pour les pr√©dictions !
                    """)
                
                with col_interp2:
                    if second_variable is not None:
                        st.info(f"""
                        ü•à **Deuxi√®me variable cl√© :**
                        **{second_variable['Variable']}** ({second_variable['Pourcentage']:.1f}% de l'impact)
                        
                        Compl√®te efficacement la premi√®re !
                        """)
                
                # Score Out-of-Bag si disponible
                if hasattr(rf_model, 'oob_score_') and rf_model.oob_score_ is not None:
                    st.info(f"üìä **Score Out-of-Bag (validation interne)** : {rf_model.oob_score_:.1%}")
            
            elif best_model_name == "R√©gression Logistique":
                lr_model = results[best_model_name]['model']
                coeffs = lr_model.coef_[0] if len(lr_model.coef_.shape) > 1 else lr_model.coef_
                
                coeff_df = pd.DataFrame({
                    'Variable': feature_cols,
                    'Coefficient': coeffs,
                    'Impact_Abs': np.abs(coeffs),
                    'Direction': ['Positif ‚¨ÜÔ∏è' if x > 0 else 'N√©gatif ‚¨áÔ∏è' for x in coeffs]
                }).sort_values('Impact_Abs', ascending=True)
                
                fig_coeff = px.bar(
                    coeff_df,
                    x='Coefficient',
                    y='Variable',
                    orientation='h',
                    title="üìä Coefficients de la R√©gression Logistique",
                    color='Coefficient',
                    color_continuous_scale='RdBu',
                    text='Direction'
                )
                
                fig_coeff.add_vline(x=0, line_dash="dash", line_color="gray")
                fig_coeff.update_layout(height=400)
                
                st.plotly_chart(fig_coeff, use_container_width=True)
                
                # Interpr√©tation des coefficients
                st.markdown("**üìù Interpr√©tation :**")
                st.write("‚Ä¢ **Coefficient positif** : Augmente la probabilit√© de la classe cible")
                st.write("‚Ä¢ **Coefficient n√©gatif** : Diminue la probabilit√© de la classe cible")
                st.write("‚Ä¢ **Plus le coefficient est grand en valeur absolue**, plus l'impact est fort")
            
            # =============================================
            # ANALYSE DE CAS CONCRETS
            # =============================================
            
            st.markdown("### üîç **Analyse de 3 Cas Concrets**")
            st.markdown("*Exemples r√©els pour comprendre le comportement du mod√®le*")
            
            # S√©lectionner 3 cas int√©ressants
            y_pred_proba_best = results[best_model_name]['y_pred_proba']
            y_pred_best = results[best_model_name]['y_pred']
            
            # Crit√®res : 1 tr√®s confiant correct, 1 incertain, 1 erreur
            confidence_scores = np.max(y_pred_proba_best, axis=1)
            correct_predictions = y_pred_best == y_test
            
            cases_to_show = []
            
            # Cas 1 : Pr√©diction tr√®s confiante et correcte
            high_conf_correct = np.where((confidence_scores > 0.8) & correct_predictions)[0]
            if len(high_conf_correct) > 0:
                cases_to_show.append(("‚úÖ Pr√©diction Excellente", high_conf_correct[0], "green"))
            
            # Cas 2 : Pr√©diction incertaine
            uncertain = np.where((confidence_scores > 0.4) & (confidence_scores < 0.7))[0]
            if len(uncertain) > 0:
                cases_to_show.append(("ü§î Cas Limite", uncertain[0], "orange"))
            
            # Cas 3 : Erreur de pr√©diction
            incorrect = np.where(~correct_predictions)[0]
            if len(incorrect) > 0:
                cases_to_show.append(("‚ùå Erreur du Mod√®le", incorrect[0], "red"))
            
            # Affichage des cas
            for i, (title, idx, color) in enumerate(cases_to_show):
                with st.container():
                    st.markdown(f"""
                    <div style="border: 2px solid {color}; border-radius: 10px; padding: 15px; margin: 10px 0;">
                    <h4>{title}</h4>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    col_profile, col_prediction = st.columns([1, 1])
                    
                    with col_profile:
                        st.markdown("**üë§ Profil :**")
                        individual_features = X_test.iloc[idx] if hasattr(X_test, 'iloc') else X_test[idx]
                        
                        if hasattr(individual_features, 'items'):
                            for feature, value in individual_features.items():
                                if feature in encoders:
                                    try:
                                        value_decoded = encoders[feature].inverse_transform([int(value)])[0]
                                        st.write(f"‚Ä¢ **{feature}** : {value_decoded}")
                                    except:
                                        st.write(f"‚Ä¢ **{feature}** : {value}")
                                else:
                                    st.write(f"‚Ä¢ **{feature}** : {value}")
                        else:
                            for j, feature in enumerate(feature_cols):
                                value = individual_features[j]
                                if feature in encoders:
                                    try:
                                        value_decoded = encoders[feature].inverse_transform([int(value)])[0]
                                        st.write(f"‚Ä¢ **{feature}** : {value_decoded}")
                                    except:
                                        st.write(f"‚Ä¢ **{feature}** : {value}")
                                else:
                                    st.write(f"‚Ä¢ **{feature}** : {value}")
                    
                    with col_prediction:
                        st.markdown("**üéØ Pr√©diction :**")
                        predicted_proba = y_pred_proba_best[idx]
                        predicted_class = y_pred_best[idx]
                        actual_class = y_test[idx] if hasattr(y_test, '__getitem__') else y_test.iloc[idx]
                        
                        # D√©coder les classes
                        if target_encoder:
                            predicted_name = target_encoder.inverse_transform([predicted_class])[0]
                            actual_name = target_encoder.inverse_transform([actual_class])[0]
                        else:
                            predicted_name = str(predicted_class)
                            actual_name = str(actual_class)
                        
                        confidence = np.max(predicted_proba) * 100
                        st.write(f"‚Ä¢ **Pr√©dite** : {predicted_name}")
                        st.write(f"‚Ä¢ **Confiance** : {confidence:.1f}%")
                        st.write(f"‚Ä¢ **R√©elle** : {actual_name}")
                        
                        # Explication simple
                        if best_model_name == "Random Forest":
                            top_features = feature_importance_df.tail(2)['Variable'].values
                            st.markdown("**üí° Facteurs cl√©s :**")
                            for feature in top_features:
                                st.write(f"‚Ä¢ {feature}")
            
            st.success("‚ú® **Ces exemples montrent comment le mod√®le 'raisonne' sur des cas r√©els !**")
            
            st.markdown("### üîç **Quelles variables sont les plus importantes ?**")
            
            # Feature importance pour Random Forest
            if "Random Forest" in results and best_model_name == "Random Forest":
                importances = results["Random Forest"]['model'].feature_importances_
                feature_importance_df = pd.DataFrame({
                    'Variable': feature_cols,
                    'Importance': importances
                }).sort_values('Importance', ascending=True)
                
                fig_importance = px.bar(
                    feature_importance_df,
                    x='Importance',
                    y='Variable',
                    orientation='h',
                    title="Importance des Variables (Random Forest)",
                    color='Importance',
                    color_continuous_scale='Viridis'
                )
                
                st.plotly_chart(fig_importance, use_container_width=True)
            
            # Pour les autres mod√®les, coefficients
            elif best_model_name == "R√©gression Logistique":
                coeffs = results["R√©gression Logistique"]['model'].coef_[0]
                coeff_df = pd.DataFrame({
                    'Variable': feature_cols,
                    'Coefficient': np.abs(coeffs)
                }).sort_values('Coefficient', ascending=True)
                
                fig_coeff = px.bar(
                    coeff_df,
                    x='Coefficient',
                    y='Variable',
                    orientation='h',
                    title="Importance des Variables (R√©gression Logistique)",
                    color='Coefficient',
                    color_continuous_scale='Plasma'
                )
                
                st.plotly_chart(fig_coeff, use_container_width=True)
            
            # =============================================
            # MATRICE DE CONFUSION INTERACTIVE
            # =============================================
            
            st.markdown("### üéØ **Matrice de Confusion : O√π le mod√®le se trompe-t-il ?**")
            
            from sklearn.metrics import confusion_matrix
            
            # Matrice pour le meilleur mod√®le
            cm = confusion_matrix(y_test, results[best_model_name]['y_pred'])
            
            # Labels originaux si encodage
            if target_encoder:
                labels = target_encoder.classes_
            else:
                labels = np.unique(y_encoded)
            
            # Visualisation interactive
            fig_cm = px.imshow(
                cm,
                text_auto=True,
                aspect="auto",
                color_continuous_scale='Blues',
                title=f"Matrice de Confusion - {best_model_name}",
                labels=dict(x="Pr√©dictions", y="Vraies Valeurs", color="Nombre"),
                x=labels,
                y=labels
            )
            
            fig_cm.update_layout(
                width=500,
                height=500
            )
            
            st.plotly_chart(fig_cm, use_container_width=True)
            
            # Calcul de la pr√©cision par classe
            diag = np.diag(cm)
            row_sums = cm.sum(axis=1)
            class_accuracy = diag / row_sums
            
            st.markdown("**Pr√©cision par classe :**")
            for i, label in enumerate(labels):
                st.metric(f"Classe '{label}'", f"{class_accuracy[i]:.1%}")
            
            # =============================================
            # RECOMMANDATIONS FINALES
            # =============================================
            
            st.markdown("### üí° **Recommandations & Prochaines √âtapes**")
            
            col_rec1, col_rec2 = st.columns(2)
            
            with col_rec1:
                st.markdown("**üéØ Pour am√©liorer le mod√®le :**")
                if best_accuracy < 0.8:
                    st.write("‚Ä¢ Collecter plus de donn√©es")
                    st.write("‚Ä¢ Ajouter des variables explicatives")
                    st.write("‚Ä¢ Essayer des techniques d'ensemble")
                else:
                    st.write("‚Ä¢ Le mod√®le est d√©j√† performant !")
                    st.write("‚Ä¢ Valider sur de nouvelles donn√©es")
                    st.write("‚Ä¢ D√©ployer en production")
            
            with col_rec2:
                st.markdown("**üíº Applications pratiques :**")
                st.write("‚Ä¢ Cibler les campagnes de sensibilisation")
                st.write("‚Ä¢ Identifier les utilisateurs √† risque")
                st.write("‚Ä¢ Personnaliser les contenus √©ducatifs")
                st.write("‚Ä¢ Optimiser les strat√©gies de communication")
            
            # R√©sum√© final
            st.success(f"""
            üéâ **Analyse Termin√©e avec Succ√®s !**
            
            ‚úÖ **Meilleur mod√®le** : {best_model_name}  
            ‚úÖ **Performance** : {best_accuracy:.1%} d'accuracy  
            ‚úÖ **Dataset** : {len(ml_data)} observations analys√©es  
            ‚úÖ **Variables** : {len(feature_cols)} caract√©ristiques utilis√©es
            """)
        
        else:
            st.error("‚ùå Aucun mod√®le n'a pu √™tre entra√Æn√© avec succ√®s")
    
    # =============================================
    # SECTION √âDUCATIVE
    # =============================================
    
    with st.expander("üìö **Comprendre le Machine Learning en 3 minutes**", expanded=False):
        st.markdown("""
        ### ü§î **Qu'est-ce qu'on fait exactement ?**
        
        **1. üéØ L'objectif :**
        - On veut pr√©dire une caract√©ristique (ex: "a vu un DeepFake") 
        - √Ä partir d'autres informations (√¢ge, genre, etc.)
        
        **2. üîß Comment √ßa marche :**
        - L'algorithme analyse les donn√©es d'entra√Ænement
        - Il trouve des "patterns" (motifs r√©currents)
        - Il utilise ces patterns pour pr√©dire de nouveaux cas
        
        **3. üìä Les m√©triques importantes :**
        - **Accuracy** : % de pr√©dictions correctes
        - **AUC** : Capacit√© √† distinguer les classes (0.5 = hasard, 1.0 = parfait)
        - **Courbe ROC** : Visualise la qualit√© du mod√®le
        
        **4. ‚ö†Ô∏è Les pi√®ges √† √©viter :**
        - **Surapprentissage** : Le mod√®le m√©morise au lieu d'apprendre
        - **Sous-apprentissage** : Le mod√®le est trop simple
        - **Biais** : Le mod√®le reproduit les biais des donn√©es
        """)
    
    # Message final si pas encore lanc√©
    if 'results' not in locals():
        st.info("üëÜ **Cliquez sur 'LANCER L'ANALYSE COMPL√àTE' pour voir la magie op√©rer !**")

# =============================================
# ONGLET 4 - SUITE COMPL√àTE D'ANALYSE PERSONAS
# =============================================

with tab4:
    st.title("üé≠ Suite d'Analyse Personas DeepFakes")
    st.markdown("*Dashboard ex√©cutif pour l'analyse comportementale et la strat√©gie*")
    
    # =============================================
    # CHARGEMENT DES DONN√âES AVEC CACHE AVANC√â
    # =============================================
    @st.cache_data(ttl=3600)  # Cache 1h
    def load_personas_data():
        try:
            url = 'https://raw.githubusercontent.com/Gnatey/M-moire_Deepfake/refs/heads/main/quantitatif.csv'
            df_personas = pd.read_csv(url, encoding='utf-8')
            
            # Nettoyage et enrichissement automatique
            df_personas['Nom_Display'] = df_personas['Nom'].apply(
                lambda x: 'üë§ Anonyme' if x == 'Anonyme' else x
            )
            
            # Score de risque calcul√©
            df_personas['Score_Risque'] = df_personas.apply(calculate_risk_score, axis=1)
            
            return df_personas
        except Exception as e:
            st.error(f"Erreur chargement : {str(e)}")
            return pd.DataFrame()
    
    def calculate_risk_score(persona):
        """Calcule un score de risque de 1-10"""
        score = 5  # Base
        
        # Facteur √¢ge
        if any(age in str(persona['Tranche d\'√¢ge']) for age in ['16-17', '20-25']):
            score += 2  # Jeunes plus expos√©s
        elif any(age in str(persona['Tranche d\'√¢ge']) for age in ['45-55']):
            score += 1  # Exp√©rience mod√©r√©e
        
        # Facteur m√©fiance (invers√© = moins m√©fiant = plus de risque)
        if 'Extr√™mement' in str(persona['Niveau de m√©fiance']):
            score -= 2
        elif 'Tr√®s m√©fiant' in str(persona['Niveau de m√©fiance']):
            score -= 1
        elif 'Moyennement' in str(persona['Niveau de m√©fiance']):
            score += 1
        
        # Facteur usage
        if persona['Fr√©quence d\'utilisation par jour'] == '1h':
            score += 1
        
        return max(1, min(10, score))
    
    df_personas = load_personas_data()
    
    if df_personas.empty:
        st.warning("‚ö†Ô∏è Donn√©es indisponibles")
        st.stop()
    
    # =============================================
    # CSS AVANC√â POUR INTERFACE PREMIUM
    # =============================================
    st.markdown("""
    <style>
    /* Variables CSS */
    :root {
        --primary-color: #667eea;
        --secondary-color: #764ba2;
        --accent-color: #f093fb;
        --danger-color: #f5576c;
        --success-color: #4ecdc4;
        --warning-color: #feca57;
        --dark-color: #2c3e50;
    }
    
    /* Cartes flip am√©lior√©es */
    .flip-card {
        background-color: transparent;
        width: 100%;
        height: 320px;
        perspective: 1000px;
        margin: 15px 0;
        border-radius: 20px;
    }
    
    .flip-card-inner {
        position: relative;
        width: 100%;
        height: 100%;
        text-align: center;
        transition: transform 0.8s cubic-bezier(0.175, 0.885, 0.32, 1.275);
        transform-style: preserve-3d;
        cursor: pointer;
    }
    
    .flip-card:hover .flip-card-inner {
        transform: rotateY(180deg);
    }
    
    .flip-card-front, .flip-card-back {
        position: absolute;
        width: 100%;
        height: 100%;
        -webkit-backface-visibility: hidden;
        backface-visibility: hidden;
        border-radius: 20px;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
        padding: 25px;
        box-sizing: border-box;
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255,255,255,0.1);
    }
    
    .flip-card-front {
        background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
        color: white;
    }
    
    .flip-card-back {
        background: linear-gradient(135deg, var(--accent-color) 0%, var(--danger-color) 100%);
        color: white;
        transform: rotateY(180deg);
    }
    
    .persona-photo {
        width: 90px;
        height: 90px;
        border-radius: 50%;
        margin: 0 auto 20px;
        background: rgba(255,255,255,0.2);
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 2.5rem;
        border: 3px solid rgba(255,255,255,0.3);
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    }
    
    .cluster-badge {
        position: absolute;
        top: 15px;
        right: 15px;
        background: rgba(255,255,255,0.95);
        color: #333;
        padding: 8px 15px;
        border-radius: 25px;
        font-size: 0.75rem;
        font-weight: bold;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    
    .risk-indicator {
        position: absolute;
        top: 15px;
        left: 15px;
        width: 40px;
        height: 40px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        font-weight: bold;
        font-size: 0.8rem;
        color: white;
        box-shadow: 0 2px 10px rgba(0,0,0,0.2);
    }
    
    .metric-mini {
        background: rgba(255,255,255,0.15);
        border-radius: 12px;
        padding: 12px;
        margin: 8px 0;
        text-align: center;
        backdrop-filter: blur(5px);
        border: 1px solid rgba(255,255,255,0.1);
    }
    
    .insight-card {
        background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
        border-radius: 20px;
        padding: 25px;
        color: white;
        margin: 15px 0;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255,255,255,0.1);
    }
    
    .dashboard-metric {
        background: white;
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        border-left: 5px solid var(--primary-color);
    }
    
    .strategy-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 15px;
        padding: 20px;
        color: white;
        margin: 10px 0;
        border-left: 5px solid rgba(255,255,255,0.3);
        backdrop-filter: blur(10px);
    }
    
    .comparison-container {
        display: flex;
        gap: 20px;
        margin: 20px 0;
    }
    
    .persona-comparison {
        flex: 1;
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        border-radius: 15px;
        padding: 20px;
        color: white;
        min-height: 200px;
    }
    
    .filter-container {
        background: rgba(255,255,255,0.05);
        border-radius: 15px;
        padding: 20px;
        margin: 20px 0;
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255,255,255,0.1);
    }
    
    .kpi-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 15px;
        margin: 20px 0;
    }
    
    .animated-counter {
        font-size: 2rem;
        font-weight: bold;
        color: var(--primary-color);
        text-align: center;
    }
    
    /* Animations */
    @keyframes slideInUp {
        from {
            opacity: 0;
            transform: translateY(30px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
    
    .slide-in {
        animation: slideInUp 0.6s ease-out;
    }
    
    /* Responsive design */
    @media (max-width: 768px) {
        .flip-card {
            height: 280px;
        }
        .comparison-container {
            flex-direction: column;
        }
    }
    </style>
    """, unsafe_allow_html=True)
    
    # =============================================
    # CLUSTERING INTELLIGENT AVANC√â
    # =============================================
    def assign_advanced_cluster(persona):
        """Clustering bas√© sur algorithme multi-crit√®res"""
        age_text = str(persona['Tranche d\'√¢ge'])
        job_text = str(persona['M√©tier']).lower()
        trust_text = str(persona['Niveau de m√©fiance'])
        education = str(persona['Niveau d\'√©tude'])
        usage = str(persona['Fr√©quence d\'utilisation par jour'])
        
        # Tech Experts (crit√®res stricts)
        if any(keyword in job_text for keyword in ['d√©veloppeur', 'dev', 'it', 'tech', 'informatique', 'chef de projet it']):
            return "üíª Tech Experts"
        
        # Digital Natives (jeunes + usage intensif)
        elif any(age in age_text for age in ['16-17', '20-25']) and 'm√©fiant' in trust_text.lower():
            return "üßí Digital Natives"
        
        # Sceptiques Experts (exp√©rience + m√©fiance √©lev√©e)
        elif ('45-55' in age_text or '40-50' in age_text) and 'Tr√®s m√©fiant' in trust_text:
            return "üéØ Sceptiques Experts"
        
        # Decision Makers (cadres + √©ducation sup√©rieure)
        elif persona['Cat√©gorie socio-professionnelle'] == 'Cadre' and education in ['Bac+5', 'Doctorat']:
            return "üëî Decision Makers"
        
        # Vuln√©rables (usage √©lev√© + faible m√©fiance)
        elif usage == '1h' and 'Moyennement' in trust_text:
            return "‚ö†Ô∏è Profils √† Risque"
        
        # √âducateurs (enseignement)
        elif 'enseignant' in job_text or 'chercheur' in job_text:
            return "üéì √âducateurs"
        
        # Default: Observateurs
        else:
            return "üëÅÔ∏è Observateurs"
    
    # Application du clustering avanc√©
    df_personas['Cluster_Advanced'] = df_personas.apply(assign_advanced_cluster, axis=1)
    
    # =============================================
    # NAVIGATION PAR ONGLETS AVANC√âS
    # =============================================
    tab_main, tab_analytics, tab_comparison, tab_strategy, tab_simulator, tab_export = st.tabs([
        "üè† Dashboard Principal", 
        "üìä Analytics Avanc√©es", 
        "‚öñÔ∏è Comparateur", 
        "üéØ Strat√©gies", 
        "üîÆ Simulateur",
        "üì• Export Premium"
    ])
    
    # =============================================
    # ONGLET 1: DASHBOARD PRINCIPAL
    # =============================================
    with tab_main:
        # KPIs Ex√©cutifs en temps r√©el
        st.markdown("### üìä KPIs Ex√©cutifs")
        
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        
        with col1:
            total_personas = len(df_personas)
            st.metric("üë• Total Personas", total_personas, delta="+2 vs last month")
        
        with col2:
            high_risk = len(df_personas[df_personas['Score_Risque'] >= 7])
            risk_pct = (high_risk / total_personas) * 100
            st.metric("üö® Profils √† Risque", f"{high_risk} ({risk_pct:.0f}%)")
        
        with col3:
            tech_experts = len(df_personas[df_personas['Cluster_Advanced'] == 'üíª Tech Experts'])
            st.metric("üíª Tech Experts", tech_experts)
        
        with col4:
            avg_age = df_personas['Tranche d\'√¢ge'].apply(
                lambda x: int(x.split(':')[1].split('-')[0].strip()) if ':' in str(x) and '-' in str(x) else 30
            ).mean()
            st.metric("üë§ √Çge Moyen", f"{avg_age:.0f} ans")
        
        with col5:
            heavy_users = len(df_personas[df_personas['Fr√©quence d\'utilisation par jour'] == '1h'])
            st.metric("‚è∞ Gros Utilisateurs", heavy_users)
        
        with col6:
            trust_score = len(df_personas[df_personas['Niveau de m√©fiance'].str.contains('Tr√®s m√©fiant', na=False)])
            trust_pct = (trust_score / total_personas) * 100
            st.metric("üõ°Ô∏è Tr√®s M√©fiants", f"{trust_pct:.0f}%")
        
        # Filtres Avanc√©s
        st.markdown("### üîç Filtres Intelligents")
        
        with st.container():
            st.markdown('<div class="filter-container">', unsafe_allow_html=True)
            
            col_f1, col_f2, col_f3, col_f4 = st.columns(4)
            
            with col_f1:
                cluster_filter = st.multiselect(
                    "üéØ Profils :",
                    options=df_personas['Cluster_Advanced'].unique(),
                    default=df_personas['Cluster_Advanced'].unique()
                )
            
            with col_f2:
                risk_filter = st.select_slider(
                    "‚ö†Ô∏è Niveau de Risque :",
                    options=["Tous", "Faible (1-3)", "Mod√©r√© (4-6)", "√âlev√© (7-10)"],
                    value="Tous"
                )
            
            with col_f3:
                age_filter = st.multiselect(
                    "üë§ Tranches d'√¢ge :",
                    options=df_personas['Tranche d\'√¢ge'].unique(),
                    default=df_personas['Tranche d\'√¢ge'].unique()
                )
            
            with col_f4:
                show_anonymous = st.checkbox("Inclure Anonymes", True)
                real_time = st.checkbox("üîÑ Temps R√©el", False)
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Application des filtres
        filtered_df = df_personas[
            (df_personas['Cluster_Advanced'].isin(cluster_filter)) &
            (df_personas['Tranche d\'√¢ge'].isin(age_filter))
        ]
        
        if risk_filter != "Tous":
            if risk_filter == "Faible (1-3)":
                filtered_df = filtered_df[filtered_df['Score_Risque'] <= 3]
            elif risk_filter == "Mod√©r√© (4-6)":
                filtered_df = filtered_df[(filtered_df['Score_Risque'] >= 4) & (filtered_df['Score_Risque'] <= 6)]
            elif risk_filter == "√âlev√© (7-10)":
                filtered_df = filtered_df[filtered_df['Score_Risque'] >= 7]
        
        if not show_anonymous:
            filtered_df = filtered_df[filtered_df['Nom'] != 'Anonyme']
        
        # Photos premium par d√©faut
        premium_photos = {
            'Cl√©mence Dupont': 'üë©‚Äçüíº',
            'Alain Airom': 'üë®‚Äçüíª', 
            'Rabiaa ZITOUNI': 'üë©‚Äçüè´',
            'Marie Moreau': 'üë©‚Äçüíº',
            'Thomas Dubois': 'üë®‚Äçüíº',
            'Pierre Martin': 'üë®‚Äçüíº',
            'Isabelle Petit': 'üë©‚Äçüíº',
            'Alexandre Garcia': 'üë®‚Äçüíª',
            'Nicolas Bernard': 'üë®‚Äçüíº',
            'Sophie Laurent': 'üë©‚Äçüéì',
            'Camille Simon': 'üë©‚Äçüíª',
            'Elodie Roux': 'üëß',
            'Jean-Michel Leroy': 'üë®‚Äçüéì'
        }
        
        # Couleurs par cluster avanc√©
        advanced_cluster_colors = {
            "üíª Tech Experts": "#3498db",
            "üßí Digital Natives": "#e74c3c", 
            "üéØ Sceptiques Experts": "#f39c12",
            "üëî Decision Makers": "#9b59b6",
            "‚ö†Ô∏è Profils √† Risque": "#e67e22",
            "üéì √âducateurs": "#2ecc71",
            "üëÅÔ∏è Observateurs": "#95a5a6"
        }
        
        # Fonction pour g√©n√©rer les cartes premium
        def generate_premium_card(persona, cluster_color):
            name = persona['Nom_Display']
            photo = premium_photos.get(persona['Nom'], 'üë§') if persona['Nom'] != 'Anonyme' else '‚ùì'
            
            # Informations simplifi√©es pour le recto
            age = persona['Tranche d\'√¢ge'].replace('Estim√© : ', '')
            job = persona['M√©tier'][:30] + "..." if len(persona['M√©tier']) > 30 else persona['M√©tier']
            location = persona['Localisation'].split(',')[0]
            cluster = persona['Cluster_Advanced']
            risk_score = persona['Score_Risque']
            
            # Couleur du risque
            if risk_score >= 7:
                risk_color = "#e74c3c"
                risk_icon = "üö®"
            elif risk_score >= 4:
                risk_color = "#f39c12"
                risk_icon = "‚ö†Ô∏è"
            else:
                risk_color = "#2ecc71"
                risk_icon = "‚úÖ"
            
            # Informations d√©taill√©es pour le verso
            citation = persona['Citation-cl√©'][:85] + "..." if len(persona['Citation-cl√©']) > 85 else persona['Citation-cl√©']
            platforms = persona['Plateformes jug√©es risqu√©es'].split(',')[0] if pd.notna(persona['Plateformes jug√©es risqu√©es']) else "N/A"
            trust_level = persona['Niveau de m√©fiance'][:20] + "..." if len(persona['Niveau de m√©fiance']) > 20 else persona['Niveau de m√©fiance']
            education = persona['Niveau d\'√©tude']
            usage = persona['Fr√©quence d\'utilisation par jour']
            
            return f"""
            <div class="flip-card slide-in">
                <div class="flip-card-inner">
                    <div class="flip-card-front">
                        <div class="cluster-badge" style="background: {cluster_color};">{cluster.split()[0]}</div>
                        <div class="risk-indicator" style="background: {risk_color};">
                            {risk_icon}<br><span style="font-size: 0.7rem;">{risk_score}</span>
                        </div>
                        <div class="persona-photo">{photo}</div>
                        <h3 style="margin: 15px 0; font-size: 1.3rem; font-weight: 600;">{name}</h3>
                        <div class="metric-mini">
                            <strong>üë§ {age}</strong>
                        </div>
                        <div class="metric-mini">
                            <strong>üíº {job}</strong>
                        </div>
                        <div class="metric-mini">
                            <strong>üìç {location}</strong>
                        </div>
                    </div>
                    <div class="flip-card-back">
                        <h4 style="margin-bottom: 15px;">üí¨ Profil D√©taill√©</h4>
                        <p style="font-style: italic; margin: 15px 0; font-size: 0.9rem;">"{citation}"</p>
                        <div class="metric-mini">
                            <strong>üéØ {trust_level}</strong>
                        </div>
                        <div class="metric-mini">
                            <strong>üì± Plateforme: {platforms}</strong>
                        </div>
                        <div class="metric-mini">
                            <strong>üéì {education}</strong>
                        </div>
                        <div class="metric-mini">
                            <strong>‚è± Usage: {usage}</strong>
                        </div>
                        <div style="margin-top: 15px; font-size: 0.8rem; opacity: 0.8;">
                            Survolez pour voir les d√©tails
                        </div>
                    </div>
                </div>
            </div>
            """
        
        # Galerie premium avec recherche
        st.markdown("### üé¥ Galerie Premium des Personas")
        
        # Barre de recherche
        col_search, col_sort = st.columns([3, 1])
        with col_search:
            search_term = st.text_input("üîç Rechercher un persona...", placeholder="Nom, m√©tier, citation...")
        with col_sort:
            sort_by = st.selectbox("Trier par :", ["Nom", "Score de Risque", "√Çge", "Cluster"])
        
        # Application de la recherche
        if search_term:
            mask = (
                filtered_df['Nom'].str.contains(search_term, case=False, na=False) |
                filtered_df['M√©tier'].str.contains(search_term, case=False, na=False) |
                filtered_df['Citation-cl√©'].str.contains(search_term, case=False, na=False)
            )
            filtered_df = filtered_df[mask]
        
        # Tri
        if sort_by == "Score de Risque":
            filtered_df = filtered_df.sort_values('Score_Risque', ascending=False)
        elif sort_by == "√Çge":
            filtered_df = filtered_df.sort_values('Tranche d\'√¢ge')
        elif sort_by == "Cluster":
            filtered_df = filtered_df.sort_values('Cluster_Advanced')
        else:
            filtered_df = filtered_df.sort_values('Nom')
        
        # Affichage des r√©sultats
        if len(filtered_df) == 0:
            st.warning("Aucun persona trouv√© avec ces crit√®res")
        else:
            st.success(f"‚ú® {len(filtered_df)} personas trouv√©s")
            
            # Grille responsive (3 cartes par ligne)
            for i in range(0, len(filtered_df), 3):
                cols = st.columns(3)
                for j, col in enumerate(cols):
                    if i + j < len(filtered_df):
                        persona = filtered_df.iloc[i + j]
                        cluster_color = advanced_cluster_colors.get(persona['Cluster_Advanced'], "#95a5a6")
                        
                        with col:
                            st.markdown(
                                generate_premium_card(persona, cluster_color),
                                unsafe_allow_html=True
                            )
        
        # Insights temps r√©el
        if real_time:
            with st.container():
                st.markdown("### ‚ö° Insights Temps R√©el")
                
                col_rt1, col_rt2, col_rt3 = st.columns(3)
                
                with col_rt1:
                    st.markdown(f"""
                    <div class="insight-card">
                        <h4>üéØ Cluster Dominant</h4>
                        <p>{filtered_df['Cluster_Advanced'].mode().iloc[0] if len(filtered_df) > 0 else 'N/A'}</p>
                        <p>Repr√©sente {len(filtered_df[filtered_df['Cluster_Advanced'] == filtered_df['Cluster_Advanced'].mode().iloc[0]]) if len(filtered_df) > 0 else 0} personas</p>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col_rt2:
                    avg_risk = filtered_df['Score_Risque'].mean() if len(filtered_df) > 0 else 0
                    risk_trend = "üìà" if avg_risk > 5 else "üìâ" if avg_risk < 4 else "‚û°Ô∏è"
                    st.markdown(f"""
                    <div class="insight-card">
                        <h4>‚ö†Ô∏è Risque Moyen</h4>
                        <p style="font-size: 2rem;">{avg_risk:.1f}/10 {risk_trend}</p>
                        <p>Tendance du segment s√©lectionn√©</p>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col_rt3:
                    top_platform = "Facebook"  # Placeholder pour logique complexe
                    st.markdown(f"""
                    <div class="insight-card">
                        <h4>üì± Plateforme Critique</h4>
                        <p style="font-size: 1.5rem;">{top_platform}</p>
                        <p>La plus mentionn√©e comme risqu√©e</p>
                    </div>
                    """, unsafe_allow_html=True)
    
    # =============================================
    # ONGLET 2: ANALYTICS AVANC√âES
    # =============================================
    with tab_analytics:
        st.markdown("### üìä Analytics Comportementales Avanc√©es")
        
        # Matrice de corr√©lation avanc√©e
        st.subheader("üîó Analyse de Corr√©lations")
        
        # Pr√©paration des donn√©es pour corr√©lation
        numeric_data = df_personas.copy()
        
        # Conversion des variables cat√©gorielles en num√©riques
        label_encoders = {}
        for col in ['Cluster_Advanced', 'Niveau de m√©fiance', 'Tranche d\'√¢ge', 'Cat√©gorie socio-professionnelle']:
            if col in numeric_data.columns:
                le = pd.Categorical(numeric_data[col]).codes
                numeric_data[f'{col}_encoded'] = le
        
        # Ajout de scores calcul√©s
        numeric_data['Usage_Score'] = numeric_data['Fr√©quence d\'utilisation par jour'].map({
            '0 min': 0, '< 30 min': 1, '‚âà 30 min': 2, '1h': 3
        }).fillna(1)
        
        numeric_data['Education_Score'] = numeric_data['Niveau d\'√©tude'].map({
            'En cours': 1, 'Bac+3': 3, 'Bac+5': 5, 'Doctorat': 7
        }).fillna(3)
        
        # S√©lection des variables pour la corr√©lation
        corr_columns = ['Score_Risque', 'Usage_Score', 'Education_Score', 'Cluster_Advanced_encoded', 'Niveau de m√©fiance_encoded']
        available_corr_columns = [col for col in corr_columns if col in numeric_data.columns]
        
        if len(available_corr_columns) > 1:
            corr_matrix = numeric_data[available_corr_columns].corr()
            
            # Heatmap interactive
            fig_corr = px.imshow(
                corr_matrix,
                text_auto=True,
                aspect="auto",
                color_continuous_scale='RdBu_r',
                zmin=-1, zmax=1,
                title="Matrice de Corr√©lation des Variables Comportementales"
            )
            fig_corr.update_layout(height=500)
            st.plotly_chart(fig_corr, use_container_width=True)
        
        # Distribution des scores de risque
        col_dist1, col_dist2 = st.columns(2)
        
        with col_dist1:
            st.subheader("üìà Distribution des Scores de Risque")
            
            fig_hist = px.histogram(
                df_personas,
                x='Score_Risque',
                nbins=10,
                title="R√©partition des Scores de Risque",
                color_discrete_sequence=['#667eea']
            )
            fig_hist.update_layout(
                xaxis_title="Score de Risque (1-10)",
                yaxis_title="Nombre de Personas"
            )
            st.plotly_chart(fig_hist, use_container_width=True)
        
        with col_dist2:
            st.subheader("üéØ Analyse par Cluster")
            
            cluster_risk = df_personas.groupby('Cluster_Advanced')['Score_Risque'].mean().sort_values(ascending=False)
            
            fig_cluster_risk = px.bar(
                x=cluster_risk.index,
                y=cluster_risk.values,
                title="Score de Risque Moyen par Cluster",
                color=cluster_risk.values,
                color_continuous_scale='Reds'
            )
            fig_cluster_risk.update_layout(
                xaxis_title="Cluster",
                yaxis_title="Score de Risque Moyen",
                xaxis_tickangle=-45
            )
            st.plotly_chart(fig_cluster_risk, use_container_width=True)
        
        # Analyse g√©ographique
        st.subheader("üó∫Ô∏è Analyse G√©ographique")
        
        # Extraction des villes
        df_personas['Ville'] = df_personas['Localisation'].str.split(',').str[0].str.strip()
        geo_analysis = df_personas.groupby('Ville').agg({
            'Nom': 'count',
            'Score_Risque': 'mean',
            'Cluster_Advanced': lambda x: x.mode().iloc[0] if len(x) > 0 else 'N/A'
        }).rename(columns={'Nom': 'Count', 'Score_Risque': 'Risque_Moyen', 'Cluster_Advanced': 'Cluster_Dominant'})
        
        col_geo1, col_geo2 = st.columns(2)
        
        with col_geo1:
            fig_geo_count = px.bar(
                geo_analysis.reset_index(),
                x='Ville',
                y='Count',
                title="Nombre de Personas par Ville",
                color='Count',
                color_continuous_scale='Blues'
            )
            fig_geo_count.update_layout(xaxis_tickangle=-45)
            st.plotly_chart(fig_geo_count, use_container_width=True)
        
        with col_geo2:
            fig_geo_risk = px.scatter(
                geo_analysis.reset_index(),
                x='Count',
                y='Risque_Moyen',
                size='Count',
                color='Cluster_Dominant',
                hover_name='Ville',
                title="Risque vs Population par Ville",
                color_discrete_map=advanced_cluster_colors
            )
            st.plotly_chart(fig_geo_risk, use_container_width=True)
        
        # Analyse temporelle simul√©e
        st.subheader("üìÖ Tendances Temporelles (Simulation)")
        
        # G√©n√©ration de donn√©es temporelles fictives
        dates = pd.date_range('2024-01-01', '2024-12-31', freq='M')
        np.random.seed(42)
        
        trend_data = pd.DataFrame({
            'Date': dates,
            'Nouveaux_Personas': np.random.poisson(2, len(dates)),
            'Score_Risque_Moyen': 5 + np.random.normal(0, 0.5, len(dates)),
            'Tech_Experts_Pct': 20 + np.random.normal(0, 3, len(dates))
        })
        
        fig_trends = px.line(
            trend_data,
            x='Date',
            y=['Score_Risque_Moyen', 'Tech_Experts_Pct'],
            title="√âvolution des M√©triques Cl√©s (2024)",
            labels={'value': 'Valeur', 'variable': 'M√©trique'}
        )
        st.plotly_chart(fig_trends, use_container_width=True)
    
    # =============================================
    # ONGLET 3: COMPARATEUR DE PERSONAS
    # =============================================
    with tab_comparison:
        st.markdown("### ‚öñÔ∏è Comparateur Avanc√© de Personas")
        
        # S√©lection des personas √† comparer
        col_select1, col_select2, col_select3 = st.columns(3)
        
        with col_select1:
            persona1 = st.selectbox(
                "Persona 1 :",
                options=df_personas['Nom'].tolist(),
                key="comp_persona1"
            )
        
        with col_select2:
            persona2 = st.selectbox(
                "Persona 2 :",
                options=df_personas['Nom'].tolist(),
                index=1 if len(df_personas) > 1 else 0,
                key="comp_persona2"
            )
        
        with col_select3:
            persona3 = st.selectbox(
                "Persona 3 (optionnel) :",
                options=["Aucun"] + df_personas['Nom'].tolist(),
                key="comp_persona3"
            )
        
        if persona1 and persona2:
            # R√©cup√©ration des donn√©es
            p1_data = df_personas[df_personas['Nom'] == persona1].iloc[0]
            p2_data = df_personas[df_personas['Nom'] == persona2].iloc[0]
            p3_data = None
            if persona3 != "Aucun":
                p3_data = df_personas[df_personas['Nom'] == persona3].iloc[0]
            
            # Comparaison visuelle
            comparison_personas = [p1_data, p2_data]
            if p3_data is not None:
                comparison_personas.append(p3_data)
            
            # Tableau de comparaison
            st.subheader("üìã Tableau Comparatif")
            
            comparison_data = []
            attributes = [
                'Nom', 'Tranche d\'√¢ge', 'M√©tier', 'Cluster_Advanced', 
                'Score_Risque', 'Niveau de m√©fiance', 'Niveau d\'√©tude',
                'Fr√©quence d\'utilisation par jour'
            ]
            
            for attr in attributes:
                row = {'Attribut': attr}
                for i, persona in enumerate(comparison_personas):
                    row[f'Persona {i+1}'] = persona[attr]
                comparison_data.append(row)
            
            comparison_df = pd.DataFrame(comparison_data)
            st.dataframe(comparison_df, use_container_width=True, hide_index=True)
            
            # Radar chart comparatif
            st.subheader("üï∏Ô∏è Profil Radar Comparatif")
            
            # Pr√©paration des donn√©es pour le radar
            categories = ['Score Risque', 'Usage Digital', 'Niveau M√©fiance', '√âducation', 'Exp√©rience Pro']
            
            def calculate_radar_scores(persona):
                usage_score = {'0 min': 1, '< 30 min': 2, '‚âà 30 min': 3, '1h': 4}.get(persona['Fr√©quence d\'utilisation par jour'], 2)
                trust_score = 1
                if "Extr√™mement" in str(persona['Niveau de m√©fiance']): trust_score = 5
                elif "Tr√®s m√©fiant" in str(persona['Niveau de m√©fiance']): trust_score = 4
                elif "M√©fiant" in str(persona['Niveau de m√©fiance']): trust_score = 3
                elif "Mod√©r√©ment" in str(persona['Niveau de m√©fiance']): trust_score = 2
                
                edu_score = {'En cours': 2, 'Bac+3': 3, 'Bac+5': 4, 'Doctorat': 5}.get(persona['Niveau d\'√©tude'], 3)
                exp_score = 3  # Score basique, peut √™tre enrichi
                
                return [persona['Score_Risque'], usage_score, trust_score, edu_score, exp_score]
            
            fig_radar = go.Figure()
            
            colors = ['#667eea', '#f093fb', '#4ecdc4']
            for i, persona in enumerate(comparison_personas):
                scores = calculate_radar_scores(persona)
                fig_radar.add_trace(go.Scatterpolar(
                    r=scores,
                    theta=categories,
                    fill='toself',
                    name=persona['Nom'],
                    line_color=colors[i]
                ))
            
            fig_radar.update_layout(
                polar=dict(
                    radialaxis=dict(visible=True, range=[0, 5])
                ),
                showlegend=True,
                title="Comparaison Multi-Dimensionnelle",
                height=500
            )
            
            st.plotly_chart(fig_radar, use_container_width=True)
            
            # Recommandations comparatives
            st.subheader("üí° Recommandations Comparatives")
            
            for i, persona in enumerate(comparison_personas):
                cluster = persona['Cluster_Advanced']
                risk_score = persona['Score_Risque']
                
                if risk_score >= 7:
                    rec_color = "#e74c3c"
                    rec_level = "PRIORIT√â √âLEV√âE"
                elif risk_score >= 4:
                    rec_color = "#f39c12"
                    rec_level = "ATTENTION MOD√âR√âE"
                else:
                    rec_color = "#2ecc71"
                    rec_level = "SURVEILLANCE L√âG√àRE"
                
                st.markdown(f"""
                <div style="border: 2px solid {rec_color}; border-radius: 15px; padding: 20px; margin: 10px 0;">
                    <h4>{persona['Nom']} - {rec_level}</h4>
                    <p><strong>Cluster:</strong> {cluster}</p>
                    <p><strong>Score de Risque:</strong> {risk_score}/10</p>
                    <p><strong>Action Recommand√©e:</strong> 
                    {"Formation technique avanc√©e" if cluster == "üíª Tech Experts" else
                     "Sensibilisation adapt√©e √† l'√¢ge" if cluster == "üßí Digital Natives" else
                     "Communication institutionnelle" if cluster == "üéØ Sceptiques Experts" else
                     "Formation en entreprise" if cluster == "üëî Decision Makers" else
                     "Surveillance renforc√©e" if cluster == "‚ö†Ô∏è Profils √† Risque" else
                     "Partenariat √©ducatif" if cluster == "üéì √âducateurs" else
                     "Sensibilisation g√©n√©rale"}</p>
                </div>
                """, unsafe_allow_html=True)
    
    # =============================================
    # ONGLET 4: G√âN√âRATEUR DE STRAT√âGIES
    # =============================================
    with tab_strategy:
        st.markdown("### üéØ G√©n√©rateur de Strat√©gies Personnalis√©es")
        
        # S√©lection du mode strat√©gique
        strategy_mode = st.radio(
            "Mode d'analyse :",
            options=["Par Cluster", "Par Persona Individuel", "Strat√©gie Globale"],
            horizontal=True
        )
        
        if strategy_mode == "Par Cluster":
            selected_cluster = st.selectbox(
                "S√©lectionnez un cluster :",
                options=df_personas['Cluster_Advanced'].unique()
            )
            
            cluster_data = df_personas[df_personas['Cluster_Advanced'] == selected_cluster]
            
            # Analyse du cluster
            st.subheader(f"üìä Analyse du Cluster : {selected_cluster}")
            
            col_stats1, col_stats2, col_stats3 = st.columns(3)
            
            with col_stats1:
                st.metric("Taille du Segment", len(cluster_data))
                avg_risk = cluster_data['Score_Risque'].mean()
                st.metric("Risque Moyen", f"{avg_risk:.1f}/10")
            
            with col_stats2:
                dominant_age = cluster_data['Tranche d\'√¢ge'].mode().iloc[0] if len(cluster_data) > 0 else "N/A"
                st.metric("√Çge Dominant", dominant_age)
                heavy_users = len(cluster_data[cluster_data['Fr√©quence d\'utilisation par jour'] == '1h'])
                st.metric("Gros Utilisateurs", heavy_users)
            
            with col_stats3:
                dominant_edu = cluster_data['Niveau d\'√©tude'].mode().iloc[0] if len(cluster_data) > 0 else "N/A"
                st.metric("√âducation Dominante", dominant_edu)
                very_cautious = len(cluster_data[cluster_data['Niveau de m√©fiance'].str.contains('Tr√®s m√©fiant', na=False)])
                st.metric("Tr√®s M√©fiants", very_cautious)
            
            # Strat√©gies personnalis√©es par cluster
            cluster_strategies = {
                "üíª Tech Experts": {
                    "objectif": "Partenariat et Co-innovation",
                    "approche": "Technique et Collaborative",
                    "actions": [
                        "ü§ù Cr√©er un programme d'ambassadeurs techniques",
                        "üîß Impliquer dans le d√©veloppement d'outils de d√©tection",
                        "üìö Proposer des formations avanc√©es en IA/ML",
                        "üí¨ Organiser des hackathons anti-deepfakes",
                        "üåê Cr√©er une communaut√© technique d√©di√©e"
                    ],
                    "kpis": ["Nombre d'ambassadeurs", "Outils d√©velopp√©s", "Participation aux √©v√©nements"],
                    "budget": "√âlev√© (50k-100k ‚Ç¨)",
                    "timeline": "6-12 mois",
                    "canaux": ["GitHub", "Stack Overflow", "LinkedIn", "Conf√©rences tech"]
                },
                "üßí Digital Natives": {
                    "objectif": "√âducation Pr√©ventive et Virale",
                    "approche": "Gamifi√©e et Sociale",
                    "actions": [
                        "üéÆ D√©velopper un jeu mobile √©ducatif",
                        "üì± Cr√©er du contenu TikTok/Instagram √©ducatif",
                        "üè´ Int√©grer dans les programmes scolaires",
                        "üë• Programme de peer-to-peer education",
                        "üèÜ Concours de d√©tection de deepfakes"
                    ],
                    "kpis": ["Engagement sur r√©seaux", "Taux de completion jeu", "Port√©e virale"],
                    "budget": "Mod√©r√© (20k-50k ‚Ç¨)",
                    "timeline": "3-6 mois",
                    "canaux": ["TikTok", "Instagram", "Discord", "√âtablissements scolaires"]
                },
                "üéØ Sceptiques Experts": {
                    "objectif": "Validation Scientifique et Transparence",
                    "approche": "Factuelle et Institutionnelle",
                    "actions": [
                        "üìä Publier des √©tudes scientifiques rigoureuses",
                        "üé§ Organiser des conf√©rences d'experts",
                        "üì∫ Interventions dans les m√©dias traditionnels",
                        "üîç D√©monstrations techniques publiques",
                        "üìñ Guide m√©thodologique de v√©rification"
                    ],
                    "kpis": ["Citations scientifiques", "Couverture m√©diatique", "Taux de confiance"],
                    "budget": "√âlev√© (40k-80k ‚Ç¨)",
                    "timeline": "6-18 mois",
                    "canaux": ["Presse traditionnelle", "Conf√©rences", "Publications scientifiques"]
                },
                "üëî Decision Makers": {
                    "objectif": "Formation Corporate et Compliance",
                    "approche": "Professionnelle et Strat√©gique",
                    "actions": [
                        "üíº Webinaires C-Level sur les risques",
                        "üìã Audit de vuln√©rabilit√© entreprise",
                        "üéì Certification anti-deepfakes",
                        "üìä Dashboard de monitoring en temps r√©el",
                        "‚öñÔ∏è Guide de compliance l√©gale"
                    ],
                    "kpis": ["Entreprises form√©es", "Audits r√©alis√©s", "Certifications d√©livr√©es"],
                    "budget": "Tr√®s √©lev√© (100k+ ‚Ç¨)",
                    "timeline": "12-24 mois",
                    "canaux": ["LinkedIn", "√âv√©nements B2B", "Presse √©conomique"]
                },
                "‚ö†Ô∏è Profils √† Risque": {
                    "objectif": "Protection Renforc√©e et Surveillance",
                    "approche": "Pr√©ventive et Protective",
                    "actions": [
                        "üö® Syst√®me d'alertes personnalis√©es",
                        "üì± App mobile de v√©rification rapide",
                        "üë• R√©seau de soutien communautaire",
                        "üìö Formation simplifi√©e et accessible",
                        "üîí Outils de protection personnelle"
                    ],
                    "kpis": ["R√©duction des incidents", "Adoption des outils", "Satisfaction utilisateur"],
                    "budget": "Mod√©r√© (30k-60k ‚Ç¨)",
                    "timeline": "3-9 mois",
                    "canaux": ["SMS", "Email", "Applications mobiles", "Support t√©l√©phonique"]
                },
                "üéì √âducateurs": {
                    "objectif": "Partenariat √âducatif et Diffusion",
                    "approche": "P√©dagogique et Collaborative",
                    "actions": [
                        "üìö Curriculum anti-deepfakes pour √©coles",
                        "üë®‚Äçüè´ Formation des formateurs",
                        "üî¨ Projet de recherche collaborative",
                        "üìñ Ressources p√©dagogiques gratuites",
                        "üèÜ Prix de l'innovation √©ducative"
                    ],
                    "kpis": ["√âtablissements partenaires", "Enseignants form√©s", "√âl√®ves touch√©s"],
                    "budget": "Mod√©r√© (25k-50k ‚Ç¨)",
                    "timeline": "6-12 mois",
                    "canaux": ["R√©seaux √©ducatifs", "Conf√©rences p√©dagogiques", "Plateformes e-learning"]
                },
                "üëÅÔ∏è Observateurs": {
                    "objectif": "Sensibilisation Douce et Progressive",
                    "approche": "Accessible et Non-intrusive",
                    "actions": [
                        "üì∫ Campagne de sensibilisation grand public",
                        "‚ùì FAQ interactive et accessible",
                        "üì∞ Articles de presse vulgaris√©s",
                        "üé• T√©moignages et cas concrets",
                        "üì± Notifications push √©ducatives"
                    ],
                    "kpis": ["Port√©e de la campagne", "Engagement contenu", "Changement de perception"],
                    "budget": "Standard (15k-30k ‚Ç¨)",
                    "timeline": "3-6 mois",
                    "canaux": ["Facebook", "YouTube", "Presse g√©n√©raliste", "TV"]
                }
            }
            
            if selected_cluster in cluster_strategies:
                strategy = cluster_strategies[selected_cluster]
                
                st.subheader("üéØ Strat√©gie Recommand√©e")
                
                # Carte strat√©gique
                st.markdown(f"""
                <div class="strategy-card">
                    <h3>üìã {strategy['objectif']}</h3>
                    <p><strong>üé® Approche:</strong> {strategy['approche']}</p>
                    <p><strong>üí∞ Budget Estim√©:</strong> {strategy['budget']}</p>
                    <p><strong>‚è± Timeline:</strong> {strategy['timeline']}</p>
                    <p><strong>üì¢ Canaux Prioritaires:</strong> {', '.join(strategy['canaux'])}</p>
                </div>
                """, unsafe_allow_html=True)
                
                # Actions d√©taill√©es
                col_actions, col_kpis = st.columns(2)
                
                with col_actions:
                    st.markdown("**üöÄ Plan d'Actions:**")
                    for action in strategy['actions']:
                        st.markdown(f"‚Ä¢ {action}")
                
                with col_kpis:
                    st.markdown("**üìä KPIs de Succ√®s:**")
                    for kpi in strategy['kpis']:
                        st.markdown(f"‚Ä¢ {kpi}")
                
                # Simulateur de ROI
                st.subheader("üíπ Simulateur de ROI")
                
                col_roi1, col_roi2, col_roi3 = st.columns(3)
                
                with col_roi1:
                    budget_input = st.number_input("Budget (‚Ç¨)", min_value=1000, max_value=200000, value=30000)
                
                with col_roi2:
                    duration_input = st.selectbox("Dur√©e (mois)", options=[3, 6, 12, 18, 24], index=1)
                
                with col_roi3:
                    target_reach = st.number_input("Port√©e Cible", min_value=100, max_value=100000, value=1000)
                
                # Calcul ROI simul√©
                efficiency_factor = {
                    "üíª Tech Experts": 1.5,
                    "üßí Digital Natives": 2.0,
                    "üéØ Sceptiques Experts": 1.2,
                    "üëî Decision Makers": 3.0,
                    "‚ö†Ô∏è Profils √† Risque": 1.8,
                    "üéì √âducateurs": 2.5,
                    "üëÅÔ∏è Observateurs": 1.0
                }.get(selected_cluster, 1.0)
                
                estimated_impact = (budget_input / 100) * efficiency_factor * (target_reach / 1000)
                roi_percentage = (estimated_impact / budget_input) * 100
                
                st.success(f"""
                üìà **ROI Estim√©:** {roi_percentage:.1f}%  
                üéØ **Impact Projet√©:** {estimated_impact:.0f} personas sensibilis√©s  
                üí∞ **Co√ªt par Persona:** {budget_input/max(1, estimated_impact):.2f}‚Ç¨
                """)
        
        elif strategy_mode == "Par Persona Individuel":
            selected_persona_name = st.selectbox(
                "S√©lectionnez un persona :",
                options=df_personas['Nom'].tolist()
            )
            
            persona_data = df_personas[df_personas['Nom'] == selected_persona_name].iloc[0]
            
            st.subheader(f"üë§ Strat√©gie Personnalis√©e : {persona_data['Nom']}")
            
            # Profil d√©taill√©
            col_profile1, col_profile2 = st.columns(2)
            
            with col_profile1:
                st.markdown(f"""
                **üìä Profil:**
                - **√Çge:** {persona_data['Tranche d\'√¢ge']}
                - **M√©tier:** {persona_data['M√©tier']}
                - **Cluster:** {persona_data['Cluster_Advanced']}
                - **Score de Risque:** {persona_data['Score_Risque']}/10
                """)
            
            with col_profile2:
                st.markdown(f"""
                **üéØ Comportement:**
                - **M√©fiance:** {persona_data['Niveau de m√©fiance']}
                - **Usage:** {persona_data['Fr√©quence d\'utilisation par jour']}
                - **√âducation:** {persona_data['Niveau d\'√©tude']}
                - **Localisation:** {persona_data['Localisation']}
                """)
            
            # Citation personnalis√©e
            st.markdown(f"""
            <div class="insight-card">
                <h4>üí¨ Citation Repr√©sentative</h4>
                <p style="font-style: italic;">"{persona_data['Citation-cl√©']}"</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Recommandations personnalis√©es
            risk_score = persona_data['Score_Risque']
            cluster = persona_data['Cluster_Advanced']
            
            if risk_score >= 8:
                priority = "üö® URGENTE"
                color = "#e74c3c"
                recommendations = [
                    "Formation imm√©diate en d√©tection de deepfakes",
                    "Installation d'outils de v√©rification",
                    "R√©duction de l'exposition aux plateformes risqu√©es",
                    "Suivi mensuel personnalis√©"
                ]
            elif risk_score >= 6:
                priority = "‚ö†Ô∏è √âLEV√âE"
                color = "#f39c12"
                recommendations = [
                    "Session de sensibilisation cibl√©e",
                    "Acc√®s aux ressources √©ducatives",
                    "Guidance sur les bonnes pratiques",
                    "Suivi trimestriel"
                ]
            elif risk_score >= 4:
                priority = "üìä MOD√âR√âE"
                color = "#3498db"
                recommendations = [
                    "Contenu √©ducatif adapt√© au profil",
                    "Participation aux webinaires",
                    "Acc√®s aux guides de v√©rification",
                    "Suivi semestriel"
                ]
            else:
                priority = "‚úÖ PR√âVENTIVE"
                color = "#2ecc71"
                recommendations = [
                    "Maintien de la vigilance actuelle",
                    "Mise √† jour p√©riodique des connaissances",
                    "R√¥le d'ambassadeur potentiel",
                    "Suivi annuel"
                ]
            
            st.markdown(f"""
            <div style="border: 3px solid {color}; border-radius: 15px; padding: 25px; margin: 20px 0;">
                <h3>üéØ Priorit√© d'Action: {priority}</h3>
                <h4>üìã Recommandations Personnalis√©es:</h4>
            """, unsafe_allow_html=True)
            
            for rec in recommendations:
                st.markdown(f"‚Ä¢ {rec}")
            
            st.markdown("</div>", unsafe_allow_html=True)
        
        elif strategy_mode == "Strat√©gie Globale":
            st.subheader("üåç Strat√©gie Globale Anti-Deepfakes")
            
            # Vue d'ensemble des clusters
            cluster_overview = df_personas['Cluster_Advanced'].value_counts()
            
            col_overview1, col_overview2 = st.columns(2)
            
            with col_overview1:
                fig_cluster_pie = px.pie(
                    values=cluster_overview.values,
                    names=cluster_overview.index,
                    title="R√©partition des Segments",
                    color_discrete_map=advanced_cluster_colors
                )
                st.plotly_chart(fig_cluster_pie, use_container_width=True)
            
            with col_overview2:
                # Matrice priorit√©/impact
                cluster_priority = df_personas.groupby('Cluster_Advanced').agg({
                    'Score_Risque': 'mean',
                    'Nom': 'count'
                }).rename(columns={'Score_Risque': 'Risque_Moyen', 'Nom': 'Taille'})
                
                fig_matrix = px.scatter(
                    cluster_priority.reset_index(),
                    x='Taille',
                    y='Risque_Moyen',
                    size='Taille',
                    color='Cluster_Advanced',
                    hover_name='Cluster_Advanced',
                    title="Matrice Impact/Priorit√©",
                    labels={'Taille': 'Taille du Segment', 'Risque_Moyen': 'Risque Moyen'},
                    color_discrete_map=advanced_cluster_colors
                )
                st.plotly_chart(fig_matrix, use_container_width=True)
            
            # Plan strat√©gique global
            st.markdown("### üìà Plan Strat√©gique Global (18 mois)")
            
            phases = {
                "Phase 1 (0-6 mois) - URGENCE": {
                    "objectif": "Traiter les profils √† haut risque",
                    "cibles": ["‚ö†Ô∏è Profils √† Risque", "üßí Digital Natives"],
                    "budget": "40% du budget total",
                    "actions": [
                        "Lancement campagne d'urgence",
                        "D√©veloppement outils de protection",
                        "Formation express des √©ducateurs"
                    ]
                },
                "Phase 2 (6-12 mois) - EXPANSION": {
                    "objectif": "√âlargir la sensibilisation",
                    "cibles": ["üëÅÔ∏è Observateurs", "üéì √âducateurs"],
                    "budget": "35% du budget total",
                    "actions": [
                        "Campagne grand public",
                        "Partenariats √©ducatifs",
                        "D√©veloppement contenu viral"
                    ]
                },
                "Phase 3 (12-18 mois) - EXCELLENCE": {
                    "objectif": "Construire l'√©cosyst√®me expert",
                    "cibles": ["üíª Tech Experts", "üëî Decision Makers", "üéØ Sceptiques Experts"],
                    "budget": "25% du budget total",
                    "actions": [
                        "Programme d'ambassadeurs",
                        "Certification professionnelle",
                        "Innovation collaborative"
                    ]
                }
            }
            
            for phase_name, phase_data in phases.items():
                st.markdown(f"""
                <div class="strategy-card">
                    <h4>{phase_name}</h4>
                    <p><strong>üéØ Objectif:</strong> {phase_data['objectif']}</p>
                    <p><strong>üë• Cibles:</strong> {', '.join(phase_data['cibles'])}</p>
                    <p><strong>üí∞ Budget:</strong> {phase_data['budget']}</p>
                    <p><strong>üöÄ Actions Cl√©s:</strong> {' ‚Ä¢ '.join(phase_data['actions'])}</p>
                </div>
                """, unsafe_allow_html=True)
            
            # KPIs globaux
            st.markdown("### üìä KPIs Strat√©giques Globaux")
            
            kpi_cols = st.columns(4)
            
            with kpi_cols[0]:
                st.metric("üéØ Port√©e Totale", "50,000+", delta="Objectif 18 mois")
            
            with kpi_cols[1]:
                st.metric("üìà R√©duction Risque", "-40%", delta="Score moyen")
            
            with kpi_cols[2]:
                st.metric("ü§ù Partenaires", "25+", delta="Organisations")
            
            with kpi_cols[3]:
                st.metric("üí∞ ROI Global", "250%", delta="Retour sur investissement")
    
    # =============================================
    # ONGLET 5: SIMULATEUR PR√âDICTIF
    # =============================================
    with tab_simulator:
        st.markdown("### üîÆ Simulateur Pr√©dictif de Comportements")
        
        st.info("üöÄ **Innovation:** Utilisez l'IA pour pr√©dire l'√©volution des comportements face aux deepfakes")
        
        # Param√®tres de simulation
        st.subheader("‚öôÔ∏è Param√®tres de Simulation")
        
        col_sim1, col_sim2, col_sim3 = st.columns(3)
        
        with col_sim1:
            sim_duration = st.selectbox("Horizon temporel", ["6 mois", "1 an", "2 ans", "5 ans"])
            intervention_level = st.slider("Niveau d'intervention", 0, 10, 5, help="0=Aucune, 10=Maximum")
        
        with col_sim2:
            tech_evolution = st.slider("√âvolution technologique", 0, 10, 7, help="Rapidit√© d'√©volution des deepfakes")
            education_budget = st.number_input("Budget √©ducation (k‚Ç¨)", 0, 1000, 100)
        
        with col_sim3:
            target_cluster = st.multiselect(
                "Clusters cibl√©s",
                options=df_personas['Cluster_Advanced'].unique(),
                default=df_personas['Cluster_Advanced'].unique()[:3]
            )
        
        if st.button("üöÄ Lancer la Simulation", type="primary"):
            # Simulation bas√©e sur des mod√®les simplifi√©s
            duration_multiplier = {"6 mois": 0.5, "1 an": 1, "2 ans": 2, "5 ans": 5}[sim_duration]
            
            # Calculs pr√©dictifs
            base_risk = df_personas['Score_Risque'].mean()
            
            # Impact de l'intervention
            risk_reduction = (intervention_level * 0.1) * duration_multiplier
            final_risk = max(1, base_risk - risk_reduction)
            
            # Impact du budget √©ducation
            education_impact = min(2, education_budget / 100)
            final_risk -= education_impact
            
            # Impact de l'√©volution technologique (augmente le risque)
            tech_impact = (tech_evolution * 0.05) * duration_multiplier
            final_risk += tech_impact
            
            final_risk = max(1, min(10, final_risk))
            
            # R√©sultats de simulation
            st.subheader("üìä R√©sultats de la Simulation")
            
            col_res1, col_res2, col_res3 = st.columns(3)
            
            with col_res1:
                risk_change = ((final_risk - base_risk) / base_risk) * 100
                st.metric(
                    "üéØ Risque Final Moyen",
                    f"{final_risk:.1f}/10",
                    delta=f"{risk_change:+.1f}%"
                )
            
            with col_res2:
                awareness_increase = intervention_level * 10 + education_impact * 15
                st.metric(
                    "üìö Augmentation Sensibilisation",
                    f"+{awareness_increase:.0f}%"
                )
            
            with col_res3:
                detection_capability = min(90, 30 + intervention_level * 6 + education_impact * 8)
                st.metric(
                    "üîç Capacit√© D√©tection",
                    f"{detection_capability:.0f}%"
                )
            
            # Graphique d'√©volution
            import numpy as np
            
            months = np.arange(0, int(duration_multiplier * 12) + 1)
            risk_evolution = []
            awareness_evolution = []
            
            for month in months:
                month_factor = month / 12
                current_risk = base_risk - (risk_reduction * month_factor) + (tech_impact * month_factor) - (education_impact * month_factor)
                current_awareness = awareness_increase * month_factor
                
                risk_evolution.append(max(1, min(10, current_risk)))
                awareness_evolution.append(min(100, current_awareness))
            
            sim_df = pd.DataFrame({
                'Mois': months,
                'Score_Risque': risk_evolution,
                'Sensibilisation': awareness_evolution
            })
            
            fig_sim = px.line(
                sim_df,
                x='Mois',
                y=['Score_Risque', 'Sensibilisation'],
                title=f"√âvolution Pr√©dite sur {sim_duration}",
                labels={'value': 'Score', 'variable': 'M√©trique', 'Mois': 'Mois'}
            )
            st.plotly_chart(fig_sim, use_container_width=True)
            
            # Analyse par cluster cibl√©
            st.subheader("üéØ Impact par Cluster Cibl√©")
            
            for cluster in target_cluster:
                cluster_data = df_personas[df_personas['Cluster_Advanced'] == cluster]
                cluster_risk = cluster_data['Score_Risque'].mean()
                
                # Facteur d'efficacit√© par cluster
                efficiency_factors = {
                    "üíª Tech Experts": 1.5,
                    "üßí Digital Natives": 2.0,
                    "üéØ Sceptiques Experts": 0.8,
                    "üëî Decision Makers": 1.2,
                    "‚ö†Ô∏è Profils √† Risque": 1.8,
                    "üéì √âducateurs": 2.2,
                    "üëÅÔ∏è Observateurs": 1.0
                }
                
                efficiency = efficiency_factors.get(cluster, 1.0)
                cluster_final_risk = max(1, cluster_risk - (risk_reduction * efficiency))
                cluster_impact = ((cluster_final_risk - cluster_risk) / cluster_risk) * 100
                
                st.markdown(f"""
                <div style="border: 2px solid {advanced_cluster_colors.get(cluster, '#95a5a6')}; 
                           border-radius: 10px; padding: 15px; margin: 10px 0;">
                    <h4>{cluster}</h4>
                    <p><strong>Risque Initial:</strong> {cluster_risk:.1f}/10</p>
                    <p><strong>Risque Final:</strong> {cluster_final_risk:.1f}/10</p>
                    <p><strong>Impact:</strong> {cluster_impact:+.1f}%</p>
                    <p><strong>Efficacit√©:</strong> {efficiency*100:.0f}%</p>
                </div>
                """, unsafe_allow_html=True)
            
            # Recommandations bas√©es sur la simulation
            st.subheader("üí° Recommandations Optimis√©es")
            
            if final_risk > base_risk:
                st.error("‚ö†Ô∏è **Alerte:** Le risque augmente malgr√© les interventions. Recommandations:")
                recommendations = [
                    "üö® Augmenter significativement le budget d'intervention",
                    "üéØ Concentrer les efforts sur les clusters les plus efficaces",
                    "‚ö° Acc√©l√©rer le d√©ploiement des solutions",
                    "ü§ù Chercher des partenariats pour amplifier l'impact"
                ]
            elif abs(final_risk - base_risk) < 0.5:
                st.warning("üìä **Stabilit√©:** Le risque reste stable. Optimisations possibles:")
                recommendations = [
                    "üîÑ R√©allouer le budget vers les clusters plus r√©ceptifs",
                    "üìà Augmenter progressivement le niveau d'intervention",
                    "üéì Renforcer les programmes √©ducatifs",
                    "üìä Am√©liorer le monitoring des r√©sultats"
                ]
            else:
                st.success("‚úÖ **Succ√®s:** R√©duction significative du risque. Continuez:")
                recommendations = [
                    "üéØ Maintenir les interventions efficaces",
                    "üì¢ √âtendre les strat√©gies qui fonctionnent",
                    "üí° Innover pour maintenir l'avantage",
                    "üåü Cr√©er des programmes d'ambassadeurs"
                ]
            
            for rec in recommendations:
                st.markdown(f"‚Ä¢ {rec}")
            
            # Export des r√©sultats de simulation
            csv_sim = sim_df.to_csv(index=False)
            st.download_button(
                "üì• Exporter R√©sultats Simulation",
                csv_sim,
                f"simulation_deepfakes_{sim_duration.replace(' ', '_')}.csv",
                "text/csv"
            )
    
    # =============================================
    # ONGLET 6: EXPORT PREMIUM
    # =============================================
    with tab_export:
        st.markdown("### üì• Suite d'Export Premium")
        
        st.info("üéØ **Exports professionnels** pour √©quipes, direction et partenaires")
        
        # Types d'exports
        export_type = st.radio(
            "Type d'export :",
            options=[
                "üìä Dashboard Ex√©cutif",
                "üë• Fiches Personas D√©taill√©es", 
                "üéØ Plan Strat√©gique Complet",
                "üìà Rapport d'Analyse",
                "üîÆ R√©sultats de Simulation"
            ]
        )
        
        # Param√®tres d'export
        col_export_params1, col_export_params2 = st.columns(2)
        
        with col_export_params1:
            selected_clusters_export = st.multiselect(
                "Clusters √† inclure :",
                options=df_personas['Cluster_Advanced'].unique(),
                default=df_personas['Cluster_Advanced'].unique()
            )
            
            include_charts = st.checkbox("Inclure les graphiques", True)
            include_recommendations = st.checkbox("Inclure les recommandations", True)
        
        with col_export_params2:
            export_format = st.selectbox(
                "Format de sortie :",
                options=["CSV", "JSON", "Markdown", "Rapport HTML"]
            )
            
            confidentiality = st.selectbox(
                "Niveau de confidentialit√© :",
                options=["Public", "Interne", "Confidentiel", "Secret"]
            )
        
        # G√©n√©ration d'exports
        if st.button("üöÄ G√©n√©rer l'Export", type="primary"):
            export_data = df_personas[df_personas['Cluster_Advanced'].isin(selected_clusters_export)]
            
            if export_type == "üìä Dashboard Ex√©cutif":
                # R√©sum√© ex√©cutif
                executive_summary = f"""
# DASHBOARD EX√âCUTIF - PERSONAS DEEPFAKES
**Confidentiel - {confidentiality}**

## üìä Synth√®se Ex√©cutive
- **Total Personas Analys√©s:** {len(export_data)}
- **Clusters Actifs:** {len(selected_clusters_export)}
- **Score de Risque Moyen:** {export_data['Score_Risque'].mean():.1f}/10
- **Profils Haute Priorit√©:** {len(export_data[export_data['Score_Risque'] >= 7])}

## üéØ Clusters Dominants
"""
                for cluster in selected_clusters_export:
                    cluster_count = len(export_data[export_data['Cluster_Advanced'] == cluster])
                    cluster_risk = export_data[export_data['Cluster_Advanced'] == cluster]['Score_Risque'].mean()
                    executive_summary += f"""
### {cluster}
- **Taille:** {cluster_count} personas
- **Risque Moyen:** {cluster_risk:.1f}/10
- **Priorit√©:** {"√âLEV√âE" if cluster_risk >= 6 else "MOD√âR√âE" if cluster_risk >= 4 else "STANDARD"}
"""
                
                if include_recommendations:
                    executive_summary += """
## üí° Recommandations Prioritaires
1. **Formation imm√©diate** des profils √† haut risque (Score ‚â• 7)
2. **Campagne cibl√©e** pour les Digital Natives
3. **Partenariat technique** avec les Tech Experts
4. **Surveillance renforc√©e** des Profils √† Risque

## üìà KPIs de Suivi Recommand√©s
- R√©duction du score de risque moyen (-20% en 6 mois)
- Augmentation de la sensibilisation (+50% en 1 an)
- Taux d'adoption des outils de d√©tection (>80%)
- Satisfaction des formations (>4.5/5)
"""
                
                if export_format == "Markdown":
                    st.download_button(
                        "‚¨áÔ∏è T√©l√©charger Dashboard Ex√©cutif",
                        executive_summary,
                        "dashboard_executif.md",
                        "text/markdown"
                    )
                elif export_format == "Rapport HTML":
                    html_content = f"""
                    <!DOCTYPE html>
                    <html>
                    <head>
                        <title>Dashboard Ex√©cutif - Personas DeepFakes</title>
                        <style>
                            body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
                            h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; }}
                            h2 {{ color: #34495e; margin-top: 30px; }}
                            .metric {{ background: #ecf0f1; padding: 15px; border-radius: 5px; margin: 10px 0; }}
                            .priority-high {{ color: #e74c3c; font-weight: bold; }}
                            .priority-moderate {{ color: #f39c12; font-weight: bold; }}
                            .priority-standard {{ color: #2ecc71; font-weight: bold; }}
                        </style>
                    </head>
                    <body>
                        {executive_summary.replace('# ', '<h1>').replace('## ', '<h2>').replace('### ', '<h3>').replace('**', '<strong>').replace('- ', '<li>')}
                    </body>
                    </html>
                    """
                    st.download_button(
                        "‚¨áÔ∏è T√©l√©charger Rapport HTML",
                        html_content,
                        "dashboard_executif.html",
                        "text/html"
                    )
            
            elif export_type == "üë• Fiches Personas D√©taill√©es":
                # Export d√©taill√© des personas
                detailed_export = export_data[[
                    'Nom', 'Cluster_Advanced', 'Score_Risque', 'Tranche d\'√¢ge', 
                    'M√©tier', 'Entreprise', 'Localisation', 'Citation-cl√©', 
                    'Niveau de m√©fiance', 'Plateformes jug√©es risqu√©es',
                    'Attentes et besoins', 'Vision future'
                ]]
                
                if export_format == "CSV":
                    csv_detailed = detailed_export.to_csv(index=False)
                    st.download_button(
                        "‚¨áÔ∏è T√©l√©charger Fiches D√©taill√©es CSV",
                        csv_detailed,
                        "fiches_personas_detaillees.csv",
                        "text/csv"
                    )
                elif export_format == "JSON":
                    json_detailed = detailed_export.to_json(orient='records', indent=2, force_ascii=False)
                    st.download_button(
                        "‚¨áÔ∏è T√©l√©charger Fiches JSON",
                        json_detailed,
                        "fiches_personas_detaillees.json",
                        "application/json"
                    )
            
            elif export_type == "üéØ Plan Strat√©gique Complet":
                # Plan strat√©gique d√©taill√©
                strategic_plan = f"""
# PLAN STRAT√âGIQUE ANTI-DEEPFAKES
**Document {confidentiality} - Version 1.0**

## üéØ Objectifs Strat√©giques

### Objectif Principal
R√©duire les risques li√©s aux deepfakes de 40% sur 18 mois √† travers une approche multi-segments.

### Objectifs Secondaires
1. Former 10,000+ personnes aux techniques de d√©tection
2. D√©velopper 5 outils de v√©rification innovants
3. Cr√©er un r√©seau de 100+ ambassadeurs
4. Atteindre 1M+ de personnes via les campagnes

## üìä Analyse des Segments Cibles

"""
                for cluster in selected_clusters_export:
                    cluster_data = export_data[export_data['Cluster_Advanced'] == cluster]
                    strategic_plan += f"""
### Segment: {cluster}
- **Taille:** {len(cluster_data)} personas
- **Risque Moyen:** {cluster_data['Score_Risque'].mean():.1f}/10
- **Strat√©gie:** {"Partenariat technique" if "Tech" in cluster else "Formation intensive" if "Risque" in cluster else "Sensibilisation √©ducative"}
- **Budget Allou√©:** {"30%" if "Tech" in cluster else "25%" if "Risque" in cluster else "15%"}
"""
                
                strategic_plan += """
## üìà Roadmap d'Ex√©cution

### Phase 1 (Mois 1-6): Urgence
- Traitement des profils √† haut risque
- D√©veloppement des outils prioritaires
- Formation des √©quipes internes

### Phase 2 (Mois 7-12): Expansion  
- D√©ploiement des campagnes grand public
- Partenariats √©ducatifs
- Am√©lioration continue des outils

### Phase 3 (Mois 13-18): Excellence
- Programme d'ambassadeurs
- Innovation collaborative
- Mesure d'impact et optimisation

## üí∞ Budget et Ressources
- **Budget Total:** 500k‚Ç¨ sur 18 mois
- **√âquipe Core:** 8 personnes full-time
- **Partenaires Strat√©giques:** 15 organisations
- **ROI Attendu:** 300% sur 3 ans
"""
                
                st.download_button(
                    "‚¨áÔ∏è T√©l√©charger Plan Strat√©gique",
                    strategic_plan,
                    "plan_strategique_deepfakes.md",
                    "text/markdown"
                )
            
            # Confirmation
            st.success(f"‚úÖ Export '{export_type}' g√©n√©r√© avec succ√®s!")
            st.info(f"üìã **Inclus:** {len(export_data)} personas, {len(selected_clusters_export)} clusters, Format {export_format}")
        
        # Exports automatiques programm√©s
        st.markdown("---")
        st.subheader("üîÑ Exports Automatiques")
        
        col_auto1, col_auto2 = st.columns(2)
        
        with col_auto1:
            auto_frequency = st.selectbox(
                "Fr√©quence automatique :",
                options=["D√©sactiv√©", "Hebdomadaire", "Mensuel", "Trimestriel"]
            )
            
            auto_recipients = st.text_area(
                "Destinataires (emails) :",
                placeholder="email1@company.com\nemail2@company.com"
            )
        
        with col_auto2:
            auto_format = st.selectbox("Format auto :", ["CSV", "JSON", "Markdown"])
            auto_confidentiality = st.selectbox("Confidentialit√© auto :", ["Interne", "Confidentiel"])
            
            if st.button("‚öôÔ∏è Configurer Exports Auto"):
                st.success("‚úÖ Configuration sauvegard√©e!")
                st.info("üìß Les exports seront envoy√©s automatiquement selon la fr√©quence choisie")
        
        # Historique des exports
        st.markdown("---")
        st.subheader("üìö Historique des Exports")
        
        # Simulation d'historique
        import datetime
        history_data = [
            {"Date": "2024-06-01", "Type": "Dashboard Ex√©cutif", "Format": "HTML", "Taille": "2.3 MB", "Statut": "‚úÖ"},
            {"Date": "2024-05-28", "Type": "Fiches Personas", "Format": "CSV", "Taille": "856 KB", "Statut": "‚úÖ"},
            {"Date": "2024-05-25", "Type": "Plan Strat√©gique", "Format": "Markdown", "Taille": "1.2 MB", "Statut": "‚úÖ"},
            {"Date": "2024-05-20", "Type": "Rapport Analyse", "Format": "JSON", "Taille": "3.1 MB", "Statut": "‚úÖ"}
        ]
        
        history_df = pd.DataFrame(history_data)
        st.dataframe(history_df, use_container_width=True, hide_index=True)
        
        # M√©triques d'export
        col_metrics1, col_metrics2, col_metrics3, col_metrics4 = st.columns(4)
        
        with col_metrics1:
            st.metric("üì• Exports Total", "47", delta="+5 ce mois")
        
        with col_metrics2:
            st.metric("üìä Dashboards", "12", delta="+2")
        
        with col_metrics3:
            st.metric("üíæ Volume Total", "156 MB", delta="+23 MB")
        
        with col_metrics4:
            st.metric("üë• Utilisateurs", "18", delta="+3")
    
    # =============================================
    # FOOTER AVEC R√âSUM√â GLOBAL
    # =============================================
    st.markdown("---")
    st.markdown("### üéØ R√©sum√© Global de l'Analyse")
    
    # M√©triques finales globales
    final_col1, final_col2, final_col3, final_col4 = st.columns(4)
    
    with final_col1:
        total_analyzed = len(df_personas)
        high_risk_count = len(df_personas[df_personas['Score_Risque'] >= 7])
        risk_percentage = (high_risk_count / total_analyzed) * 100
        
        st.markdown(f"""
        <div class="dashboard-metric">
            <h4>üé≠ Personas Analys√©s</h4>
            <p style="font-size: 2rem; color: #3498db; font-weight: bold;">{total_analyzed}</p>
            <p>üö® {high_risk_count} √† haut risque ({risk_percentage:.0f}%)</p>
        </div>
        """, unsafe_allow_html=True)
    
    with final_col2:
        clusters_count = len(df_personas['Cluster_Advanced'].unique())
        dominant_cluster = df_personas['Cluster_Advanced'].mode().iloc[0]
        
        st.markdown(f"""
        <div class="dashboard-metric">
            <h4>üéØ Segments Identifi√©s</h4>
            <p style="font-size: 2rem; color: #e74c3c; font-weight: bold;">{clusters_count}</p>
            <p>üëë Dominant: {dominant_cluster}</p>
        </div>
        """, unsafe_allow_html=True)
    
    with final_col3:
        avg_risk_global = df_personas['Score_Risque'].mean()
        risk_trend = "üìà" if avg_risk_global > 5 else "üìâ"
        
        st.markdown(f"""
        <div class="dashboard-metric">
            <h4>‚ö†Ô∏è Risque Moyen Global</h4>
            <p style="font-size: 2rem; color: #f39c12; font-weight: bold;">{avg_risk_global:.1f}/10</p>
            <p>{risk_trend} Tendance g√©n√©rale</p>
        </div>
        """, unsafe_allow_html=True)
    
    with final_col4:
        recommended_budget = int(avg_risk_global * 50000)  # Budget bas√© sur le risque
        
        st.markdown(f"""
        <div class="dashboard-metric">
            <h4>üí∞ Budget Recommand√©</h4>
            <p style="font-size: 2rem; color: #2ecc71; font-weight: bold;">{recommended_budget:,}‚Ç¨</p>
            <p>üí° Sur 18 mois</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Action finale
    st.markdown("### üöÄ Prochaines √âtapes Recommand√©es")
    
    next_steps_col1, next_steps_col2 = st.columns(2)
    
    with next_steps_col1:
        st.markdown("""
        <div class="insight-card">
            <h4>üìã Actions Imm√©diates (Cette Semaine)</h4>
            <p>‚Ä¢ Valider la strat√©gie avec la direction</p>
            <p>‚Ä¢ Allouer le budget pour les profils √† haut risque</p>
            <p>‚Ä¢ Lancer la formation des Tech Experts</p>
            <p>‚Ä¢ Planifier la campagne pour Digital Natives</p>
        </div>
        """, unsafe_allow_html=True)
    
    with next_steps_col2:
        st.markdown("""
        <div class="insight-card">
            <h4>üéØ Objectifs √† 30 Jours</h4>
            <p>‚Ä¢ Former 100+ personnes aux techniques de d√©tection</p>
            <p>‚Ä¢ D√©ployer les premiers outils de v√©rification</p>
            <p>‚Ä¢ √âtablir 5 partenariats strat√©giques</p>
            <p>‚Ä¢ Mesurer l'impact des premi√®res actions</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Call-to-action final
    st.markdown("---")
    col_cta1, col_cta2, col_cta3 = st.columns(3)
    
    with col_cta1:
        if st.button("üìä Exporter Tout", type="primary"):
            st.success("üéâ Export global en cours de g√©n√©ration...")
    
    with col_cta2:
        if st.button("üìß Partager Insights"):
            st.info("üì§ Rapport envoy√© aux parties prenantes")
    
    with col_cta3:
        if st.button("üîÑ Actualiser Donn√©es"):
            st.success("‚úÖ Donn√©es actualis√©es avec succ√®s")

# =============================================
# SECTION COMMENTAIRES
# =============================================

# =============================================
# CONFIGURATION
# =============================================
COMMENTS_FILE = "comments_advanced.csv"

# =============================================
# API GOOGLE
# =============================================
def connect_to_gsheet():
    scope = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    creds_dict = st.secrets["GSHEET_CREDS"]
    creds = Credentials.from_service_account_info(creds_dict, scopes=scope)
    client = gspread.authorize(creds)
    sheet = client.open("user").worksheet("user_data")
    return sheet

def load_users():
    sheet = connect_to_gsheet()
    data = sheet.get_all_records()
    return pd.DataFrame(data) if data else pd.DataFrame(columns=["id", "pseudo", "password"])

def save_user(pseudo, password):
    sheet = connect_to_gsheet()
    existing_users = sheet.get_all_records()
    next_id = len(existing_users) + 1
    sheet.append_row([next_id, pseudo, password])

def get_comments_sheet():
    sheet = connect_to_gsheet()
    return sheet.spreadsheet.worksheet("comments_data")

def load_comments():
    sheet = get_comments_sheet()
    data = sheet.get_all_records()
    return pd.DataFrame(data)

def save_comment(user, comment):
    sheet = get_comments_sheet()
    new_row = [str(uuid.uuid4()), user, comment, datetime.now().strftime("%Y-%m-%d %H:%M:%S")]
    sheet.append_row(new_row)

def hash_password(password):
    return hashlib.sha256(password.encode()).hexdigest()

# =============================================
# INITIALISATION SESSION
# =============================================
if 'user_logged_in' not in st.session_state:
    st.session_state.user_logged_in = False

# =============================================
# SIDEBAR : CONNEXION / INSCRIPTION
# =============================================
def handle_auth():
    st.sidebar.header("üîê Connexion rapide")
    mode = st.sidebar.radio("Choisissez une option :", ["Se connecter", "S'inscrire"])

    with st.sidebar.form(key="auth_form"):
        pseudo = st.text_input("Votre pseudo").strip()
        password = st.text_input("Mot de passe", type="password")
        submit = st.form_submit_button("Valider")

        forbidden_pseudos = {"admin", "root", "support", "moderator","cacaboudin"}

        if submit:
            if not pseudo or not password:
                st.sidebar.error("Veuillez remplir tous les champs.")
            elif not pseudo.isalnum():
                st.sidebar.error("Le pseudo ne doit contenir que des lettres et des chiffres.")
            elif len(pseudo) < 3 or len(pseudo) > 20:
                st.sidebar.error("Le pseudo doit contenir entre 3 et 20 caract√®res.")
            elif pseudo.lower() in forbidden_pseudos:
                st.sidebar.error("Ce pseudo est r√©serv√©.")
            elif len(password) < 7:
                st.sidebar.error("Le mot de passe doit contenir au moins 7 caract√®res.")
            else:
                users_df = load_users()
                existing_pseudos_lower = users_df['pseudo'].str.lower()
                hashed_pwd = hash_password(password)

                if mode == "Se connecter":
                    if pseudo.lower() in existing_pseudos_lower.values:
                        user_row = users_df.loc[existing_pseudos_lower == pseudo.lower()].iloc[0]
                        if user_row['password'] == hashed_pwd:
                            st.session_state.user_logged_in = True
                            st.session_state.user_name = user_row['pseudo']
                            st.success(f"Bienvenue {user_row['pseudo']} !")
                            st.experimental_rerun()
                        else:
                            st.sidebar.error("Mot de passe incorrect.")
                    else:
                        st.sidebar.error("Utilisateur inconnu.")

                elif mode == "S'inscrire":
                    if pseudo.lower() in existing_pseudos_lower.values:
                        st.sidebar.error("Ce pseudo est d√©j√† utilis√©.")
                    else:
                        save_user(pseudo, hashed_pwd)
                        st.success("Inscription r√©ussie, vous √™tes connect√©.")
                        st.session_state.user_logged_in = True
                        st.session_state.user_name = pseudo
                        st.experimental_rerun()

    if st.session_state.user_logged_in:
        if st.sidebar.button("Se d√©connecter"):
            for key in ["user_logged_in", "user_name"]:
                if key in st.session_state:
                    del st.session_state[key]
            st.sidebar.success("D√©connect√© avec succ√®s.")
            st.experimental_rerun()

# =============================================
# APPEL CONNEXION
# =============================================
handle_auth()

# =============================================
# SECTION COMMENTAIRES
# =============================================

st.title("üí¨ Espace Commentaires")

# Chargement des commentaires
comments_df = load_comments()

# Fonction pour supprimer un commentaire par ID dans Google Sheets
def delete_comment(comment_id):
    sheet = get_comments_sheet()
    records = sheet.get_all_records()
    for i, row in enumerate(records, start=2):  # Ligne 2 car ligne 1 = en-t√™tes
        if row["id"] == comment_id:
            sheet.delete_rows(i)
            break

# Formulaire d'ajout de commentaire (si connect√©)
if st.session_state.get("user_logged_in", False):
    with st.form(key="comment_form", clear_on_submit=True):
        comment_text = st.text_area("Votre commentaire")
        submit_comment = st.form_submit_button("üì§ Envoyer")

        if submit_comment:
            if not comment_text:
                st.warning("Merci de remplir votre commentaire.")
            else:
                save_comment(st.session_state.user_name, comment_text.strip())
                st.success("Commentaire enregistr√©!")
                st.experimental_rerun()
else:
    st.info("üîí Connectez-vous pour pouvoir laisser un commentaire.")

# Affichage des derniers commentaires
st.subheader("üìù Derniers commentaires")

if comments_df.empty:
    st.info("Aucun commentaire pour le moment.")
else:
    comments_display = comments_df.sort_values("timestamp", ascending=False)
    for idx, row in comments_display.iterrows():
        with st.container(border=True):
            st.markdown(f"**{row['user']}** - *{row['timestamp']}*")
            st.markdown(f"> {row['comment']}")

            # Bouton de suppression (visible seulement pour l'auteur)
            if st.session_state.get("user_logged_in", False) and st.session_state.get("user_name") == row["user"]:
                delete_key = f"delete_{idx}"
                confirm_key = f"confirm_delete_{idx}"

                if st.button("üóëÔ∏è Supprimer", key=delete_key):
                    st.session_state[confirm_key] = True

                if st.session_state.get(confirm_key, False):
                    st.warning("‚ö†Ô∏è Confirmation suppression")
                    if st.button("‚úÖ Oui, supprimer", key=f"confirmed_{idx}"):
                        delete_comment(row["id"])
                        st.success("Commentaire supprim√©.")
                        st.session_state[confirm_key] = False
                        st.experimental_rerun()


# =============================================
# ONGLETS EN CONSTRUCTION - MESSAGE EDITEUR
# =============================================

with tab4:
    st.markdown("### üë©‚Äçüíª MESSAGE DEVELOPPEUSE")
    col_img, col_msg = st.columns([1, 5])
    with col_img:
        st.image("images.jpeg", width=100)
    with col_msg:
        st.info("Cet onglet est en cours de r√©daction. Vous verrez des visualisations sous peu.")

with tab3:
    st.markdown("### üë©‚Äçüíª MESSAGE DEVELOPPEUSE")
    col_img, col_msg = st.columns([1, 5])
    with col_img:
        st.image("images.jpeg", width=100)
    with col_msg:
        st.info("Cet onglet est en cours de r√©daction. Il n'est pas encore finalis√©. Certaines visualisations peuvent √™tre incorrectes")
